{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8596c570-3556-4f5f-97de-426359064a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3063f52-0f1e-49eb-9426-97e6a6571ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "DATA_DIR = \"./\"\n",
    "OUTPUT_CSV = \"arugambay_virtual_bouy_data.csv\"\n",
    "\n",
    "# VIRTUAL BUOY LOCATION (Deep water point)\n",
    "TARGET_LAT = 7.0\n",
    "TARGET_LON = 82.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b581d5-8364-4e4e-9085-ce89f9c239ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Historical Data FOR Virtual Buoy Point (7.0, 82.0)...\n",
      "   Processing surf_data_2020.nc...\n",
      "   Processing surf_data_2021.nc...\n",
      "   Processing surf_data_2022.nc...\n",
      "   Processing surf_data_2023.nc...\n",
      "   Processing surf_data_2024.nc...\n",
      "   Processing surf_data_2025.nc...\n",
      " Saved 8591 rows to arugambay_virtual_bouy_data.csv\n",
      " Variables: ['time', 'u10', 'v10', 'msl', 'shts', 'mpts', 'mdts']\n",
      " Range: 2020-01-01 00:00:00 -> 2025-11-17 12:00:00\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(f\" Historical Data FOR Virtual Buoy Point ({TARGET_LAT}, {TARGET_LON})...\")\n",
    "    \n",
    "    files = sorted(glob.glob(os.path.join(DATA_DIR, \"surf_data_*.nc\")))\n",
    "    if not files:\n",
    "        print(\" No .nc files found!\")\n",
    "        return\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for f in files:\n",
    "        print(f\"   Processing {os.path.basename(f)}...\")\n",
    "        try:\n",
    "            ds = xr.open_dataset(f)\n",
    "            \n",
    "            # 1. HANDLE EXPVER (CRITICAL FOR ERA5T)\n",
    "            # If expver exists, it means we have overlapping versions. \n",
    "            # We combine them to get a single timeline.\n",
    "            if 'expver' in ds.coords:\n",
    "                # Combine consolidated (1) and preliminary (5)\n",
    "                try:\n",
    "                    ds = ds.sel(expver=1).combine_first(ds.sel(expver=5))\n",
    "                except:\n",
    "                    pass # If selection fails, it might not need combining\n",
    "\n",
    "            # 2. Select Nearest Point\n",
    "            point_ds = ds.sel(latitude=TARGET_LAT, longitude=TARGET_LON, method='nearest')\n",
    "            \n",
    "            # 3. Convert to DataFrame\n",
    "            df = point_ds.to_dataframe().reset_index()\n",
    "            \n",
    "            # 4. Keep and Rename Columns\n",
    "            # We keep the original ERA5 names so the schema is consistent\n",
    "            target_cols = ['valid_time', 'u10', 'v10', 'msl', 'shts', 'mpts', 'mdts']\n",
    "            \n",
    "            # Filter to ensure columns exist\n",
    "            existing_cols = [c for c in target_cols if c in df.columns]\n",
    "            df = df[existing_cols].rename(columns={'valid_time': 'time'})\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error reading {f}: {e}\")\n",
    "\n",
    "    # 5. Merge and Save\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs).sort_values('time').reset_index(drop=True)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        final_df = final_df.drop_duplicates(subset=['time'])\n",
    "        \n",
    "        # Handle any remaining NaNs (interpolating briefly if single steps are missing)\n",
    "        final_df = final_df.interpolate(method='linear', limit_direction='both')\n",
    "        \n",
    "        final_df.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(f\" Saved {len(final_df)} rows to {OUTPUT_CSV}\")\n",
    "        print(f\" Variables: {list(final_df.columns)}\")\n",
    "        print(f\" Range: {final_df['time'].min()} -> {final_df['time'].max()}\")\n",
    "    else:\n",
    "        print(\"Failed to build history.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
