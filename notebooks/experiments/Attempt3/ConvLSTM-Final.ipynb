{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d8c690-8afa-4139-9c81-58851aa9a45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Flatten, Dense, BatchNormalization, Dropout, PReLU\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7581d6e-4d53-4e89-9f85-ae78b99d5eae",
   "metadata": {},
   "source": [
    "### CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063b43cc-bd10-4544-964e-a7385629a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"surf_data_2020.nc\"\n",
    "SPOT_LAT, SPOT_LON = 6.8399, 81.8396  # Arugam Bay\n",
    "LOOKBACK_HOURS = 28\n",
    "LOOKAHEAD_HOURS = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Feature and target configuration\n",
    "INPUT_FEATURES = [\"u10\", \"v10\", \"msl\", \"tp\", \"shts\", \"mpts\", \"mdts\"]\n",
    "TARGET_VARS = [\"shts\", \"mpts\", \"mdts_sin\", \"mdts_cos\", \"wind_speed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a8a52-6410-47e8-9e98-24b2bf445722",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Buoy Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7cc43f-aa00-4e6a-847e-5f9548c4434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_ocean_point(ds, lat, lon):\n",
    "    \"\"\"Find nearest valid ocean point using Haversine distance.\"\"\"\n",
    "    nearest = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "    \n",
    "    if not np.isnan(nearest[\"shts\"].isel(valid_time=0)):\n",
    "        print(\"Found valid offshore point\")\n",
    "        return float(nearest.latitude), float(nearest.longitude)\n",
    "    \n",
    "    print(\"Nearest point on land. Searching for closest ocean point...\")\n",
    "    shts_data = ds[\"shts\"].isel(valid_time=0).stack(\n",
    "        point=(\"latitude\", \"longitude\")\n",
    "    ).dropna(\"point\")\n",
    "    \n",
    "    # Haversine distance\n",
    "    R = 6371  # Earth radius (km)\n",
    "    lat1, lon1 = np.radians(lat), np.radians(lon)\n",
    "    lat2 = np.radians(shts_data.latitude)\n",
    "    lon2 = np.radians(shts_data.longitude)\n",
    "    \n",
    "    dlat = (lat2 - lat1) / 2\n",
    "    dlon = (lon2 - lon1) / 2\n",
    "    \n",
    "    a = np.sin(dlat)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon)**2\n",
    "    distance = 2 * R * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    closest = shts_data.isel(point=distance.argmin())\n",
    "    return float(closest.latitude), float(closest.longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe358d4f-f0fe-45c3-bd4e-a3196252c5b9",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946b9ee7-2caf-4117-91be-f6fd29ef9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(ds, buoy_lat, buoy_lon):\n",
    "    \"\"\"Extract and engineer features for model input.\"\"\"\n",
    "    # Spatial features (full grid)\n",
    "    X = ds[INPUT_FEATURES].to_array(dim=\"channel\").transpose(\n",
    "        \"valid_time\", \"latitude\", \"longitude\", \"channel\"\n",
    "    ).values\n",
    "    \n",
    "    # Target point features\n",
    "    buoy_data = ds.sel(latitude=buoy_lat, longitude=buoy_lon)\n",
    "    \n",
    "    # Wind speed from components\n",
    "    wind_speed = np.sqrt(buoy_data[\"u10\"]**2 + buoy_data[\"v10\"]**2).values\n",
    "    \n",
    "    # Direction encoding (sine/cosine)\n",
    "    mdts_rad = np.deg2rad(buoy_data[\"mdts\"].values)\n",
    "    mdts_sin = np.sin(mdts_rad)\n",
    "    mdts_cos = np.cos(mdts_rad)\n",
    "    \n",
    "    # Target array\n",
    "    y = np.column_stack([\n",
    "        buoy_data[\"shts\"].values,\n",
    "        buoy_data[\"mpts\"].values,\n",
    "        mdts_sin,\n",
    "        mdts_cos,\n",
    "        wind_speed\n",
    "    ])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4d943-2ac9-4aef-9b5b-d06d5719bbc8",
   "metadata": {},
   "source": [
    "### 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81772652-a310-4859-b22a-434816352b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y):\n",
    "    \"\"\"Clean, normalize, and scale data.\"\"\"\n",
    "    # Count number of NaN/Inf values\n",
    "    nan_count_input = np.sum(np.isnan(X))\n",
    "    nan_count_feature = np.sum(np.isnan(y))\n",
    "    print(f\"Number of NaN values in input variables: {nan_count_input} and in feature variables: {nan_count_feature}\")\n",
    "    \n",
    "    # Remove NaN/Inf\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Normalize X per channel\n",
    "    X_scaled = np.zeros_like(X)\n",
    "    scalers_X = []\n",
    "    \n",
    "    for i in range(X.shape[-1]):\n",
    "        scaler = StandardScaler()\n",
    "        channel = X[..., i].reshape(-1, 1)\n",
    "        X_scaled[..., i] = scaler.fit_transform(channel).reshape(X[..., i].shape)\n",
    "        scalers_X.append(scaler)\n",
    "    \n",
    "    # Normalize y\n",
    "    scaler_y = StandardScaler()\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "    \n",
    "    return X_scaled, y_scaled, scalers_X, scaler_y\n",
    "\n",
    "\n",
    "def create_sequences(X, y, lookback, lookahead):\n",
    "    \"\"\"Generate sliding window sequences.\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - lookback - lookahead + 1):\n",
    "        X_seq.append(X[i:i + lookback])\n",
    "        y_seq.append(y[i + lookback + lookahead - 1])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7b439-ae38-490d-9390-772556e3cd59",
   "metadata": {},
   "source": [
    "### 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "460334e5-a26d-4b48-8e8e-cb6e1d567cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_v2(input_shape, output_dim):\n",
    "    \"\"\"Stacked ConvLSTM with more filters - better spatiotemporal learning.\"\"\"\n",
    "    model = Sequential([\n",
    "        ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        ConvLSTM2D(32, (3, 3), padding='same', return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128),\n",
    "        PReLU(),\n",
    "        Dense(64),\n",
    "        PReLU(),\n",
    "        Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4781cb-f1e6-4460-99c9-5980892ac6b9",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0222388-7164-41f4-8710-b1d14b5c0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, scaler_y, target_names):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    y_pred_scaled = model.predict(X_val, verbose=0)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_true = scaler_y.inverse_transform(y_val)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    # Overall metrics\n",
    "    mae_overall = mean_absolute_error(y_true, y_pred)\n",
    "    rmse_overall = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2_overall = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VALIDATION METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Overall | MAE={mae_overall:.4f}, RMSE={rmse_overall:.4f}, RÂ²={r2_overall:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Per-variable metrics\n",
    "    for i, var in enumerate(target_names):\n",
    "        mae_i = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        rmse_i = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))\n",
    "        r2_i = r2_score(y_true[:, i], y_pred[:, i])\n",
    "        print(f\"{var:>12s} | MAE={mae_i:.3f}, RMSE={rmse_i:.3f}, RÂ²={r2_i:.3f}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c3f96d-af98-466f-a803-557fc38f2fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Nearest point on land. Searching for closest ocean point...\n",
      "Virtual buoy: (7.00Â°N, 82.00Â°E)\n",
      "\n",
      "Engineering features...\n",
      "Spatial input shape: (1464, 21, 21, 7)\n",
      "Target shape: (1464, 5)\n",
      "Preprocessing data...\n",
      "Number of NaN values in input variables: 1528416 and in feature variables: 0\n",
      "Creating sequences (lookback=28, lookahead=1)...\n",
      "Sequence shape: X=(1436, 28, 21, 21, 7), y=(1436, 5)\n",
      "\n",
      "ðŸ—ï¸ Building Stacked ConvLSTM Model (v2)...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, 28, 21, 21, 64)    163840    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 28, 21, 21, 64)    256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 21, 21, 32)        110720    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 21, 21, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 14112)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1806464   \n",
      "                                                                 \n",
      " p_re_lu (PReLU)             (None, 128)               128       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " p_re_lu_1 (PReLU)           (None, 64)                64        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2090181 (7.97 MB)\n",
      "Trainable params: 2089989 (7.97 MB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Training for up to 15 epochs...\n",
      "Epoch 1/15\n",
      "36/36 [==============================] - 251s 7s/step - loss: 1.9421 - mae: 0.8547 - val_loss: 0.8975 - val_mae: 0.7585 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "36/36 [==============================] - 248s 7s/step - loss: 0.4700 - mae: 0.5209 - val_loss: 0.8432 - val_mae: 0.7268 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "36/36 [==============================] - 260s 7s/step - loss: 0.3882 - mae: 0.4684 - val_loss: 0.8359 - val_mae: 0.7140 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "25/36 [===================>..........] - ETA: 1:14 - loss: 0.3627 - mae: 0.4554"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining for up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     43\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m     46\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(VALIDATION_SPLIT \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_seq))\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "ds = xr.open_dataset(DATA_PATH)\n",
    "\n",
    "# Find valid buoy location\n",
    "buoy_lat, buoy_lon = find_nearest_ocean_point(ds, SPOT_LAT, SPOT_LON)\n",
    "print(f\"Virtual buoy: ({buoy_lat:.2f}Â°N, {buoy_lon:.2f}Â°E)\")\n",
    "\n",
    "# Feature engineering\n",
    "print(\"\\nEngineering features...\")\n",
    "X, y = engineer_features(ds, buoy_lat, buoy_lon)\n",
    "print(f\"Spatial input shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "X_scaled, y_scaled, scalers_X, scaler_y = preprocess_data(X, y)\n",
    "\n",
    "# Create sequences\n",
    "print(f\"Creating sequences (lookback={LOOKBACK_HOURS}, lookahead={LOOKAHEAD_HOURS})...\")\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "print(f\"Sequence shape: X={X_seq.shape}, y={y_seq.shape}\")\n",
    "\n",
    "# Build model (v2 only)\n",
    "print(\"\\nðŸ—ï¸ Building Stacked ConvLSTM Model (v2)...\")\n",
    "model = build_model_v2(X_seq.shape[1:], y_seq.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "# Train model\n",
    "print(f\"\\nTraining for up to {EPOCHS} epochs...\")\n",
    "history = model.fit(\n",
    "    X_seq, y_seq,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "val_size = int(VALIDATION_SPLIT * len(X_seq))\n",
    "evaluate_model(model, X_seq[-val_size:], y_seq[-val_size:], scaler_y, TARGET_VARS)\n",
    "\n",
    "# Forecast next time step\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FORECAST (Next 6 Hours)\")\n",
    "print(\"=\"*60)\n",
    "last_seq = np.expand_dims(X_scaled[-LOOKBACK_HOURS:], axis=0)\n",
    "forecast_scaled = model.predict(last_seq, verbose=0)\n",
    "forecast = scaler_y.inverse_transform(forecast_scaled)[0]\n",
    "\n",
    "# Reconstruct direction from sin/cos\n",
    "direction_deg = np.rad2deg(np.arctan2(forecast[2], forecast[3])) % 360\n",
    "\n",
    "print(f\"Swell Height:    {forecast[0]:.2f} m\")\n",
    "print(f\"Swell Period:    {forecast[1]:.2f} s\")\n",
    "print(f\"Swell Direction: {direction_deg:.1f}Â°\")\n",
    "print(f\"Wind Speed:      {forecast[4]:.2f} m/s ({forecast[4]*3.6:.1f} km/h)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660050c7-505a-4603-b421-0eaa1406bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Flatten, Dense, BatchNormalization, Dropout, PReLU\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import sys\n",
    "\n",
    "# Python 3.9 compatibility fix\n",
    "if sys.version_info >= (3, 10):\n",
    "    from typing import TypeAlias\n",
    "else:\n",
    "    from typing_extensions import TypeAlias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4c6070-17a8-460e-971a-921cd95b4d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒŠ IMPROVED CONVLSTM PIPELINE WITH NaN HANDLING\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‚ Loading dataset...\n",
      "\n",
      "======================================================================\n",
      "NaN DIAGNOSTICS\n",
      "======================================================================\n",
      "\n",
      "u10: âœ“ No NaNs\n",
      "\n",
      "v10: âœ“ No NaNs\n",
      "\n",
      "msl: âœ“ No NaNs\n",
      "\n",
      "shts:\n",
      "  Total NaNs: 509,472 (78.91%)\n",
      "  Affected spatial points: 348 / 441\n",
      "  Affected timesteps: 1464 / 1464\n",
      "\n",
      "mpts:\n",
      "  Total NaNs: 509,472 (78.91%)\n",
      "  Affected spatial points: 348 / 441\n",
      "  Affected timesteps: 1464 / 1464\n",
      "\n",
      "mdts:\n",
      "  Total NaNs: 509,472 (78.91%)\n",
      "  Affected spatial points: 348 / 441\n",
      "  Affected timesteps: 1464 / 1464\n",
      "======================================================================\n",
      "â­ï¸  Skipping temporal interpolation...\n",
      "âš  Nearest point on land. Searching for closest ocean point...\n",
      "ðŸ“ Virtual buoy: (7.00Â°N, 82.00Â°E)\n",
      "\n",
      "ðŸŒŠ Ocean Mask Created:\n",
      "   Valid ocean points: 93 / 441 (21.1%)\n",
      "\n",
      "ðŸ“Š Feature Extraction:\n",
      "   Input shape: (1464, 21, 21, 6)\n",
      "   Target shape: (1464, 5)\n",
      "   Input NaNs: 1,528,416\n",
      "   Target NaNs: 0\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING\n",
      "======================================================================\n",
      "âœ“ Ocean mask applied (land points â†’ 0)\n",
      "\n",
      "Remaining NaNs:\n",
      "   Input (X): 0 (0.000%)\n",
      "   Target (y): 0 (0.000%)\n",
      "\n",
      "âœ“ Scaled data | X: (1464, 21, 21, 6), y: (1464, 5)\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Creating sequences (lookback=28, lookahead=12)...\n",
      "   Sequence shape: X=(1425, 28, 21, 21, 6), y=(1425, 5)\n",
      "\n",
      "ðŸ—ï¸ Building model...\n",
      "\n",
      "ðŸš€ Training for up to 15 epochs...\n",
      "Epoch 1/15\n",
      "36/36 [==============================] - 233s 6s/step - loss: 2.2079 - mae: 0.9395 - val_loss: 1.0595 - val_mae: 0.8140 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "36/36 [==============================] - 214s 6s/step - loss: 0.5745 - mae: 0.5763 - val_loss: 1.1524 - val_mae: 0.8310 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "36/36 [==============================] - 211s 6s/step - loss: 0.5303 - mae: 0.5466 - val_loss: 1.0492 - val_mae: 0.8186 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "36/36 [==============================] - 210s 6s/step - loss: 0.4898 - mae: 0.5259 - val_loss: 1.1083 - val_mae: 0.8266 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "36/36 [==============================] - 210s 6s/step - loss: 0.5126 - mae: 0.5421 - val_loss: 1.0710 - val_mae: 0.8178 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "36/36 [==============================] - 210s 6s/step - loss: 0.4475 - mae: 0.4990 - val_loss: 1.0895 - val_mae: 0.8221 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "31/36 [========================>.....] - ETA: 27s - loss: 0.4543 - mae: 0.5067"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 365\u001b[0m\n\u001b[0;32m    359\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    360\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    361\u001b[0m     ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    362\u001b[0m ]\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸš€ Training for up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 365\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    372\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m    375\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(VALIDATION_SPLIT \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_seq))\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# ConvLSTM Wave Forecasting (Fixed NaN Handling)\n",
    "# ======================================================================\n",
    "\n",
    "# CONFIGURATION\n",
    "DATA_PATH = \"surf_data_2024.nc\"\n",
    "SPOT_LAT, SPOT_LON = 6.8399, 81.8396  # Arugam Bay\n",
    "LOOKBACK_HOURS = 28\n",
    "LOOKAHEAD_HOURS = 12\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "INPUT_FEATURES = [\"u10\", \"v10\", \"msl\", \"shts\", \"mpts\", \"mdts\"]\n",
    "TARGET_VARS = [\"shts\", \"mpts\", \"mdts_sin\", \"mdts_cos\", \"wind_speed\"]\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# IMPROVED NaN DIAGNOSTICS\n",
    "# ======================================================================\n",
    "def diagnose_nans(ds, features):\n",
    "    \"\"\"Detailed NaN analysis before preprocessing.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NaN DIAGNOSTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for var in features:\n",
    "        data = ds[var].values\n",
    "        nan_count = np.isnan(data).sum()\n",
    "        nan_pct = 100 * nan_count / data.size\n",
    "        \n",
    "        # Check if NaNs are in specific locations\n",
    "        nan_mask = np.isnan(data)\n",
    "        if nan_count > 0:\n",
    "            # Spatial distribution\n",
    "            spatial_nans = nan_mask.any(axis=0)  # Any NaN across time\n",
    "            temporal_nans = nan_mask.any(axis=(1, 2))  # Any NaN across space\n",
    "            \n",
    "            print(f\"\\n{var}:\")\n",
    "            print(f\"  Total NaNs: {nan_count:,} ({nan_pct:.2f}%)\")\n",
    "            print(f\"  Affected spatial points: {spatial_nans.sum()} / {spatial_nans.size}\")\n",
    "            print(f\"  Affected timesteps: {temporal_nans.sum()} / {len(temporal_nans)}\")\n",
    "        else:\n",
    "            print(f\"\\n{var}: âœ“ No NaNs\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# SOLUTION 1: OCEAN MASK (Recommended for Spatial Data)\n",
    "# ======================================================================\n",
    "def create_ocean_mask(ds, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Create a mask to exclude land points.\n",
    "    Points with >50% NaN values across time are considered land.\n",
    "    \"\"\"\n",
    "    # Use wave height as indicator (always NaN over land)\n",
    "    shts_data = ds[\"shts\"].values\n",
    "    nan_ratio = np.isnan(shts_data).sum(axis=0) / shts_data.shape[0]\n",
    "    ocean_mask = nan_ratio < threshold\n",
    "    \n",
    "    valid_points = ocean_mask.sum()\n",
    "    total_points = ocean_mask.size\n",
    "    \n",
    "    print(f\"\\nðŸŒŠ Ocean Mask Created:\")\n",
    "    print(f\"   Valid ocean points: {valid_points} / {total_points} ({100*valid_points/total_points:.1f}%)\")\n",
    "    \n",
    "    return ocean_mask\n",
    "\n",
    "\n",
    "def apply_ocean_mask(X, ocean_mask):\n",
    "    \"\"\"Apply ocean mask to spatial data, setting land points to 0.\"\"\"\n",
    "    X_masked = X.copy()\n",
    "    # Broadcast mask to all timesteps and channels\n",
    "    mask_3d = np.broadcast_to(ocean_mask[..., np.newaxis], X.shape)\n",
    "    X_masked[~mask_3d] = 0.0\n",
    "    return X_masked\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# SOLUTION 2: TEMPORAL INTERPOLATION (COMMENTED OUT)\n",
    "# ======================================================================\n",
    "# def interpolate_temporal_gaps(ds, features, method='linear', limit=2):\n",
    "#     \"\"\"\n",
    "#     Fill small temporal gaps using interpolation.\n",
    "#     Only fills gaps up to 'limit' consecutive NaNs.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nâ±ï¸  Interpolating temporal gaps (method={method}, max_gap={limit})...\")\n",
    "#     \n",
    "#     ds_interp = ds.copy()\n",
    "#     for var in features:\n",
    "#         original_nans = np.isnan(ds[var].values).sum()\n",
    "#         \n",
    "#         # Interpolate along time dimension\n",
    "#         ds_interp[var] = ds[var].interpolate_na(\n",
    "#             dim='valid_time', \n",
    "#             method=method,\n",
    "#             limit=limit,\n",
    "#             fill_value='extrapolate'\n",
    "#         )\n",
    "#         \n",
    "#         remaining_nans = np.isnan(ds_interp[var].values).sum()\n",
    "#         filled = original_nans - remaining_nans\n",
    "#         \n",
    "#         if filled > 0:\n",
    "#             print(f\"   {var}: Filled {filled:,} NaNs ({original_nans:,} â†’ {remaining_nans:,})\")\n",
    "#     \n",
    "#     return ds_interp\n",
    "\n",
    "# Simple function that just returns the original dataset\n",
    "def interpolate_temporal_gaps(ds, features, method='linear', limit=2):\n",
    "    print(\"â­ï¸  Skipping temporal interpolation...\")\n",
    "    return ds\n",
    "\n",
    "# ======================================================================\n",
    "# SOLUTION 3: SMART NaN REMOVAL\n",
    "# ======================================================================\n",
    "def remove_corrupted_timesteps(X, y, max_nan_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Remove timesteps where NaN ratio exceeds threshold.\n",
    "    More conservative than filling with zeros.\n",
    "    \"\"\"\n",
    "    nan_ratio_per_time = np.isnan(X).sum(axis=(1, 2, 3)) / (X.shape[1] * X.shape[2] * X.shape[3])\n",
    "    valid_mask = nan_ratio_per_time < max_nan_ratio\n",
    "    \n",
    "    removed = len(X) - valid_mask.sum()\n",
    "    if removed > 0:\n",
    "        print(f\"\\nðŸ—‘ï¸  Removed {removed} corrupted timesteps (>{max_nan_ratio*100:.0f}% NaNs)\")\n",
    "    \n",
    "    return X[valid_mask], y[valid_mask]\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# IMPROVED PREPROCESSING\n",
    "# ======================================================================\n",
    "def preprocess_data_v2(X, y, ocean_mask=None):\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing with proper NaN handling.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREPROCESSING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Apply ocean mask if provided\n",
    "    if ocean_mask is not None:\n",
    "        X = apply_ocean_mask(X, ocean_mask)\n",
    "        print(\"âœ“ Ocean mask applied (land points â†’ 0)\")\n",
    "    \n",
    "    # Step 2: Check remaining NaNs\n",
    "    nan_count_X = np.isnan(X).sum()\n",
    "    nan_count_y = np.isnan(y).sum()\n",
    "    \n",
    "    print(f\"\\nRemaining NaNs:\")\n",
    "    print(f\"   Input (X): {nan_count_X:,} ({100*nan_count_X/X.size:.3f}%)\")\n",
    "    print(f\"   Target (y): {nan_count_y:,} ({100*nan_count_y/y.size:.3f}%)\")\n",
    "    \n",
    "    # Step 3: Remove rows with NaN targets (critical!)\n",
    "    if nan_count_y > 0:\n",
    "        valid_mask = ~np.isnan(y).any(axis=1)\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        print(f\"âœ“ Removed {(~valid_mask).sum()} samples with NaN targets\")\n",
    "    \n",
    "    # Step 4: Handle remaining spatial NaNs (e.g., edge effects)\n",
    "    # Fill with spatial mean (better than 0 for model learning)\n",
    "    if nan_count_X > 0:\n",
    "        for i in range(X.shape[-1]):\n",
    "            channel = X[..., i]\n",
    "            channel_mean = np.nanmean(channel)\n",
    "            X[..., i] = np.nan_to_num(channel, nan=channel_mean)\n",
    "        print(f\"âœ“ Filled remaining input NaNs with channel means\")\n",
    "    \n",
    "    # Step 5: Scale data\n",
    "    X_scaled = np.zeros_like(X)\n",
    "    scalers_X = []\n",
    "    \n",
    "    for i in range(X.shape[-1]):\n",
    "        scaler = StandardScaler()\n",
    "        channel = X[..., i].reshape(-1, 1)\n",
    "        X_scaled[..., i] = scaler.fit_transform(channel).reshape(X[..., i].shape)\n",
    "        scalers_X.append(scaler)\n",
    "    \n",
    "    scaler_y = StandardScaler()\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "    \n",
    "    print(f\"\\nâœ“ Scaled data | X: {X_scaled.shape}, y: {y_scaled.shape}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return X_scaled, y_scaled, scalers_X, scaler_y\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# IMPROVED FEATURE ENGINEERING\n",
    "# ======================================================================\n",
    "def engineer_features_v2(ds, buoy_lat, buoy_lon):\n",
    "    \"\"\"Feature engineering with NaN tracking.\"\"\"\n",
    "    # Spatial input\n",
    "    X = ds[INPUT_FEATURES].to_array(dim=\"channel\").transpose(\n",
    "        \"valid_time\", \"latitude\", \"longitude\", \"channel\"\n",
    "    ).values\n",
    "    \n",
    "    # Target point\n",
    "    buoy_data = ds.sel(latitude=buoy_lat, longitude=buoy_lon)\n",
    "    \n",
    "    wind_speed = np.sqrt(buoy_data[\"u10\"]**2 + buoy_data[\"v10\"]**2).values\n",
    "    mdts_rad = np.deg2rad(buoy_data[\"mdts\"].values)\n",
    "    \n",
    "    y = np.column_stack([\n",
    "        buoy_data[\"shts\"].values,\n",
    "        buoy_data[\"mpts\"].values,\n",
    "        np.sin(mdts_rad),\n",
    "        np.cos(mdts_rad),\n",
    "        wind_speed\n",
    "    ])\n",
    "    \n",
    "    # Diagnostics\n",
    "    print(f\"\\nðŸ“Š Feature Extraction:\")\n",
    "    print(f\"   Input shape: {X.shape}\")\n",
    "    print(f\"   Target shape: {y.shape}\")\n",
    "    print(f\"   Input NaNs: {np.isnan(X).sum():,}\")\n",
    "    print(f\"   Target NaNs: {np.isnan(y).sum():,}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# FIND NEAREST OCEAN POINT (Same as before)\n",
    "# ======================================================================\n",
    "def find_nearest_ocean_point(ds, lat, lon):\n",
    "    \"\"\"Find nearest valid ocean point using Haversine distance.\"\"\"\n",
    "    nearest = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "    \n",
    "    if not np.isnan(nearest[\"shts\"].isel(valid_time=0)):\n",
    "        print(\"âœ“ Found valid offshore point\")\n",
    "        return float(nearest.latitude), float(nearest.longitude)\n",
    "    \n",
    "    print(\"âš  Nearest point on land. Searching for closest ocean point...\")\n",
    "    shts_data = ds[\"shts\"].isel(valid_time=0).stack(\n",
    "        point=(\"latitude\", \"longitude\")\n",
    "    ).dropna(\"point\")\n",
    "    \n",
    "    R = 6371\n",
    "    lat1, lon1 = np.radians(lat), np.radians(lon)\n",
    "    lat2 = np.radians(shts_data.latitude)\n",
    "    lon2 = np.radians(shts_data.longitude)\n",
    "    \n",
    "    dlat = (lat2 - lat1) / 2\n",
    "    dlon = (lon2 - lon1) / 2\n",
    "    \n",
    "    a = np.sin(dlat)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon)**2\n",
    "    distance = 2 * R * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    closest = shts_data.isel(point=distance.argmin())\n",
    "    return float(closest.latitude), float(closest.longitude)\n",
    "\n",
    "\n",
    "def create_sequences(X, y, lookback, lookahead):\n",
    "    \"\"\"Generate sliding window sequences.\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - lookback - lookahead + 1):\n",
    "        X_seq.append(X[i:i + lookback])\n",
    "        y_seq.append(y[i + lookback + lookahead - 1])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "def build_model_v2(input_shape, output_dim):\n",
    "    \"\"\"Stacked ConvLSTM architecture.\"\"\"\n",
    "    model = Sequential([\n",
    "        ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        ConvLSTM2D(32, (3, 3), padding='same', return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128),\n",
    "        PReLU(),\n",
    "        Dense(64),\n",
    "        PReLU(),\n",
    "        Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_val, y_val, scaler_y, target_names):\n",
    "    \"\"\"Evaluation metrics.\"\"\"\n",
    "    y_pred_scaled = model.predict(X_val, verbose=0)\n",
    "    \n",
    "    y_true = scaler_y.inverse_transform(y_val)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    mae_overall = mean_absolute_error(y_true, y_pred)\n",
    "    rmse_overall = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2_overall = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATION METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Overall | MAE={mae_overall:.4f}, RMSE={rmse_overall:.4f}, RÂ²={r2_overall:.4f}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for i, var in enumerate(target_names):\n",
    "        mae_i = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        rmse_i = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))\n",
    "        r2_i = r2_score(y_true[:, i], y_pred[:, i])\n",
    "        print(f\"{var:>12s} | MAE={mae_i:.3f}, RMSE={rmse_i:.3f}, RÂ²={r2_i:.3f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# MAIN EXECUTION\n",
    "# ======================================================================\n",
    "\n",
    "print(\"ðŸŒŠ IMPROVED CONVLSTM PIPELINE WITH NaN HANDLING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "print(\"\\nðŸ“‚ Loading dataset...\")\n",
    "ds = xr.open_dataset(DATA_PATH)\n",
    "\n",
    "# Diagnose NaNs BEFORE processing\n",
    "diagnose_nans(ds, INPUT_FEATURES)\n",
    "\n",
    "# STRATEGY 1: Interpolate small gaps\n",
    "ds = interpolate_temporal_gaps(ds, INPUT_FEATURES, method='linear', limit=2)\n",
    "\n",
    "# Find buoy location\n",
    "buoy_lat, buoy_lon = find_nearest_ocean_point(ds, SPOT_LAT, SPOT_LON)\n",
    "print(f\"ðŸ“ Virtual buoy: ({buoy_lat:.2f}Â°N, {buoy_lon:.2f}Â°E)\")\n",
    "\n",
    "# STRATEGY 2: Create ocean mask\n",
    "ocean_mask = create_ocean_mask(ds)\n",
    "\n",
    "# Feature engineering\n",
    "X, y = engineer_features_v2(ds, buoy_lat, buoy_lon)\n",
    "\n",
    "# STRATEGY 3: Improved preprocessing\n",
    "X_scaled, y_scaled, scalers_X, scaler_y = preprocess_data_v2(X, y, ocean_mask)\n",
    "\n",
    "# Create sequences\n",
    "print(f\"\\nðŸ”„ Creating sequences (lookback={LOOKBACK_HOURS}, lookahead={LOOKAHEAD_HOURS})...\")\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "print(f\"   Sequence shape: X={X_seq.shape}, y={y_seq.shape}\")\n",
    "\n",
    "# Build and train\n",
    "print(\"\\nðŸ—ï¸ Building model...\")\n",
    "model = build_model_v2(X_seq.shape[1:], y_seq.shape[1])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸš€ Training for up to {EPOCHS} epochs...\")\n",
    "history = model.fit(\n",
    "    X_seq, y_seq,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "val_size = int(VALIDATION_SPLIT * len(X_seq))\n",
    "evaluate_model(model, X_seq[-val_size:], y_seq[-val_size:], scaler_y, TARGET_VARS)\n",
    "\n",
    "# Forecast\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ”® FORECAST (Next 12 Hours)\")\n",
    "print(\"=\"*70)\n",
    "last_seq = np.expand_dims(X_scaled[-LOOKBACK_HOURS:], axis=0)\n",
    "forecast_scaled = model.predict(last_seq, verbose=0)\n",
    "forecast = scaler_y.inverse_transform(forecast_scaled)[0]\n",
    "\n",
    "direction_deg = np.rad2deg(np.arctan2(forecast[2], forecast[3])) % 360\n",
    "\n",
    "print(f\"Swell Height:    {forecast[0]:.2f} m\")\n",
    "print(f\"Swell Period:    {forecast[1]:.2f} s\")\n",
    "print(f\"Swell Direction: {direction_deg:.1f}Â°\")\n",
    "print(f\"Wind Speed:      {forecast[4]:.2f} m/s ({forecast[4]*3.6:.1f} km/h)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74985c-45b9-495a-974b-162314f3481a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
