{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4382119e-a348-4ba5-9f80-c96a23e678cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Year ConvLSTM Wave Forecasting Pipeline\n",
    "# Using 2020-2024 ERA5 Data\n",
    "# ======================================================================\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearrflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIGURATION\n",
    "DATA_YEARS = [2020, 2021, 2022, 2023, 2024]\n",
    "DATA_DIR = \"../\"  # Directory containing surf_data_YYYY.nc files\n",
    "SPOT_LAT, SPOT_LON = 6.8399, 81.8396\n",
    "\n",
    "# Training config\n",
    "LOOKBACK_HOURS = 19\n",
    "LOOKAHEAD_HOURS = 1\n",
    "BATCH_SIZE = 16  # Can increase with more data\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Data split strategy\n",
    "TRAIN_YEARS = [2020, 2021, 2022, 2023]  # 4 years training\n",
    "VAL_YEAR = 2024  # Hold out 2024 for validation (most recent)\n",
    "\n",
    "INPUT_FEATURES = [\"u10\", \"v10\", \"msl\", \"shts\", \"mpts\", \"mdts\"]\n",
    "TARGET_FEATURES = [\"shts\", \"mpts\", \"mdts\"]n.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    ConvLSTM2D, Conv3D, BatchNormalization, Input, Dropout\n",
    ")\n",
    "from tenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c85bc9-1342-4d55-9df7-41d21b78416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-YEAR DATA LOADING\n",
    "def load_multiyear_dataset(years, data_dir=\".\"):   \n",
    "    datasets = []\n",
    "    for year in years:\n",
    "        filepath = Path(data_dir) / f\"surf_data_{year}.nc\"\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            print(f\"Warning: {filepath} not found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            ds = xr.open_dataset(filepath)\n",
    "            \n",
    "            # Verify required variables exist\n",
    "            missing_vars = set(INPUT_FEATURES + TARGET_FEATURES) - set(ds.data_vars)\n",
    "            if missing_vars:\n",
    "                print(f\"{year}: Missing variables {missing_vars}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Add year attribute for tracking\n",
    "            ds.attrs['year'] = year\n",
    "            datasets.append(ds)\n",
    "            \n",
    "            print(f\"Loaded {year}: {len(ds.valid_time)} timesteps, \"\n",
    "                  f\"shape={ds[INPUT_FEATURES[0]].shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {year}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not datasets:\n",
    "        raise ValueError(\"No valid datasets loaded!\")\n",
    "    \n",
    "    # Concatenate along time dimension\n",
    "    print(\"\\n Concatenating datasets...\")\n",
    "    combined_ds = xr.concat(datasets, dim='valid_time')\n",
    "    \n",
    "    # Sort by time (important!)\n",
    "    combined_ds = combined_ds.sortby('valid_time')\n",
    "    \n",
    "    print(f\"\\n Combined dataset:\")\n",
    "    print(f\"   Total timesteps: {len(combined_ds.valid_time)}\")\n",
    "    print(f\"   Time range: {combined_ds.valid_time.values[0]} to {combined_ds.valid_time.values[-1]}\")\n",
    "    print(f\"   Spatial shape: {len(combined_ds.latitude)} Ã— {len(combined_ds.longitude)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return combined_ds\n",
    "\n",
    "\n",
    "def split_by_year(ds, train_years, val_years):\n",
    "    \"\"\"\n",
    "    Split dataset by year (better than random split for time series).\n",
    "    \"\"\"\n",
    "    train_mask = ds.valid_time.dt.year.isin(train_years)\n",
    "    val_mask = ds.valid_time.dt.year.isin(val_years)\n",
    "    \n",
    "    ds_train = ds.isel(valid_time=train_mask)\n",
    "    ds_val = ds.isel(valid_time=val_mask)\n",
    "    \n",
    "    print(f\"\\n Data Split:\")\n",
    "    print(f\"   Training years: {train_years} â†’ {len(ds_train.valid_time)} samples\")\n",
    "    print(f\"   Validation years: {val_years} â†’ {len(ds_val.valid_time)} samples\")\n",
    "    print(f\"   Split ratio: {len(ds_train.valid_time)/(len(ds_train.valid_time)+len(ds_val.valid_time)):.1%} train\")\n",
    "    \n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9c2ff8-474f-4422-b5a1-328dcc431976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED PREPROCESSING FOR MULTI-YEAR DATA\n",
    "def create_ocean_mask(ds, threshold=0.5):\n",
    "    \"\"\"Create persistent ocean mask across all years.\"\"\"\n",
    "    shts_data = ds[\"shts\"].values\n",
    "    nan_ratio = np.isnan(shts_data).sum(axis=0) / shts_data.shape[0]\n",
    "    ocean_mask = nan_ratio < threshold\n",
    "    \n",
    "    valid_points = ocean_mask.sum()\n",
    "    print(f\"\\n Ocean Mask: {valid_points}/{ocean_mask.size} points ({100*valid_points/ocean_mask.size:.1f}%)\")\n",
    "    return ocean_mask\n",
    "\n",
    "\n",
    "def engineer_frames(ds, ocean_mask):\n",
    "    \"\"\"Extract and prepare spatial frames.\"\"\"\n",
    "    X = ds[INPUT_FEATURES].to_array(dim=\"channel\").transpose(\n",
    "        \"valid_time\", \"latitude\", \"longitude\", \"channel\"\n",
    "    ).values\n",
    "    \n",
    "    y = ds[TARGET_FEATURES].to_array(dim=\"channel\").transpose(\n",
    "        \"valid_time\", \"latitude\", \"longitude\", \"channel\"\n",
    "    ).values\n",
    "    \n",
    "    # Apply ocean mask\n",
    "    mask_3d_X = np.broadcast_to(ocean_mask[..., np.newaxis], X.shape[1:])\n",
    "    mask_3d_y = np.broadcast_to(ocean_mask[..., np.newaxis], y.shape[1:])\n",
    "    \n",
    "    X[:, ~mask_3d_X] = 0.0\n",
    "    y[:, ~mask_3d_y] = 0.0\n",
    "    \n",
    "    # Fill NaNs with channel-wise means\n",
    "    for i in range(X.shape[-1]):\n",
    "        channel_mean = np.nanmean(X[..., i])\n",
    "        X[..., i] = np.nan_to_num(X[..., i], nan=channel_mean)\n",
    "    \n",
    "    for i in range(y.shape[-1]):\n",
    "        channel_mean = np.nanmean(y[..., i])\n",
    "        y[..., i] = np.nan_to_num(y[..., i], nan=channel_mean)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fit_scalers(X_train, y_train):    \n",
    "    scalers_X = []\n",
    "    for i in range(X_train.shape[-1]):\n",
    "        scaler = StandardScaler()\n",
    "        channel = X_train[..., i].reshape(-1, 1)\n",
    "        scaler.fit(channel)\n",
    "        scalers_X.append(scaler)\n",
    "    \n",
    "    scalers_y = []\n",
    "    for i in range(y_train.shape[-1]):\n",
    "        scaler = StandardScaler()\n",
    "        channel = y_train[..., i].reshape(-1, 1)\n",
    "        scaler.fit(channel)\n",
    "        scalers_y.append(scaler)\n",
    "    \n",
    "    return scalers_X, scalers_y\n",
    "\n",
    "\n",
    "def apply_scalers(X, y, scalers_X, scalers_y):\n",
    "    \"\"\"Apply pre-fitted scalers.\"\"\"\n",
    "    X_scaled = np.zeros_like(X)\n",
    "    y_scaled = np.zeros_like(y)\n",
    "    \n",
    "    for i in range(X.shape[-1]):\n",
    "        channel = X[..., i].reshape(-1, 1)\n",
    "        X_scaled[..., i] = scalers_X[i].transform(channel).reshape(X[..., i].shape)\n",
    "    \n",
    "    for i in range(y.shape[-1]):\n",
    "        channel = y[..., i].reshape(-1, 1)\n",
    "        y_scaled[..., i] = scalers_y[i].transform(channel).reshape(y[..., i].shape)\n",
    "    \n",
    "    return X_scaled, y_scaled\n",
    "\n",
    "\n",
    "def create_sequences(X, y, lookback, lookahead):\n",
    "    \"\"\"Generate sequences with progress tracking.\"\"\"\n",
    "    total_sequences = len(X) - lookback - lookahead + 1\n",
    "    print(f\"\\n Creating {total_sequences} sequences...\")\n",
    "    \n",
    "    X_seq = np.zeros((total_sequences, lookback, *X.shape[1:]), dtype=np.float32)\n",
    "    y_seq = np.zeros((total_sequences, *y.shape[1:]), dtype=np.float32)\n",
    "    \n",
    "    for i in range(total_sequences):\n",
    "        X_seq[i] = X[i:i + lookback]\n",
    "        y_seq[i] = y[i + lookback + lookahead - 1]\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"   Progress: {i+1}/{total_sequences} ({100*(i+1)/total_sequences:.1f}%)\")\n",
    "    \n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7293fd28-22c4-4630-aade-da6a50d55a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE/LOAD PREPROCESSED DATA\n",
    "def save_preprocessed_data(X_train, y_train, X_val, y_val, scalers_X, scalers_y, filename=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Save preprocessed data to avoid reprocessing.\"\"\"\n",
    "    print(f\"\\n Saving preprocessed data to {filename}...\")\n",
    "    \n",
    "    data = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'scalers_X': scalers_X,\n",
    "        'scalers_y': scalers_y\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=4)\n",
    "    \n",
    "    print(f\"âœ“ Saved {filename} ({Path(filename).stat().st_size / 1e6:.1f} MB)\")\n",
    "\n",
    "\n",
    "def load_preprocessed_data(filename=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Load preprocessed data.\"\"\"\n",
    "    print(f\"\\n Loading preprocessed data from {filename}...\")\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(\" Loaded preprocessed data\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d3fa47-cfdc-45f9-9f50-44dd90612128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL ARCHITECTURE\n",
    "def build_enhanced_model(input_shape, output_channels):\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = ConvLSTM2D(\n",
    "        128, (3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    )(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = ConvLSTM2D(\n",
    "        64, (3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = ConvLSTM2D(\n",
    "        32, (3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=True\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Conv3D prediction layer\n",
    "    x = Conv3D(\n",
    "        output_channels,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        padding='same',\n",
    "        activation='linear'\n",
    "    )(x)\n",
    "    \n",
    "    # Extract last timestep\n",
    "    outputs = x[:, -1, :, :, :]\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67fea05-d538-4aab-8404-7ed0b73f6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "def evaluate_model(model, X_val, y_val, scalers_y, target_names):\n",
    "    \"\"\"Comprehensive evaluation.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    y_pred_scaled = model.predict(X_val, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_true = np.zeros_like(y_val)\n",
    "    y_pred = np.zeros_like(y_pred_scaled)\n",
    "    \n",
    "    for i in range(y_val.shape[-1]):\n",
    "        y_true[..., i] = scalers_y[i].inverse_transform(\n",
    "            y_val[..., i].reshape(-1, 1)\n",
    "        ).reshape(y_val[..., i].shape)\n",
    "        y_pred[..., i] = scalers_y[i].inverse_transform(\n",
    "            y_pred_scaled[..., i].reshape(-1, 1)\n",
    "        ).reshape(y_pred_scaled[..., i].shape)\n",
    "    \n",
    "    # Overall metrics\n",
    "    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "    rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))\n",
    "    \n",
    "    print(f\"\\nOverall Spatial Metrics:\")\n",
    "    print(f\"   MAE:  {mae:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Per-variable metrics\n",
    "    print(f\"\\nPer-Variable Metrics:\")\n",
    "    for i, var in enumerate(target_names):\n",
    "        mae_var = mean_absolute_error(y_true[..., i].flatten(), y_pred[..., i].flatten())\n",
    "        rmse_var = np.sqrt(mean_squared_error(y_true[..., i].flatten(), y_pred[..., i].flatten()))\n",
    "        r2_var = r2_score(y_true[..., i].flatten(), y_pred[..., i].flatten())\n",
    "        \n",
    "        print(f\"   {var:>6s}: MAE={mae_var:.3f}, RMSE={rmse_var:.3f}, RÂ²={r2_var:.3f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def find_nearest_ocean_point(ds, lat, lon):\n",
    "    \"\"\"Find nearest valid ocean point.\"\"\"\n",
    "    nearest = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "    \n",
    "    if not np.isnan(nearest[\"shts\"].isel(valid_time=0)):\n",
    "        return float(nearest.latitude), float(nearest.longitude)\n",
    "    \n",
    "    shts_data = ds[\"shts\"].isel(valid_time=0).stack(\n",
    "        point=(\"latitude\", \"longitude\")\n",
    "    ).dropna(\"point\")\n",
    "    \n",
    "    R = 6371\n",
    "    lat1, lon1 = np.radians(lat), np.radians(lon)\n",
    "    lat2 = np.radians(shts_data.latitude)\n",
    "    lon2 = np.radians(shts_data.longitude)\n",
    "    \n",
    "    dlat = (lat2 - lat1) / 2\n",
    "    dlon = (lon2 - lon1) / 2\n",
    "    \n",
    "    a = np.sin(dlat)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon)**2\n",
    "    distance = 2 * R * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    closest = shts_data.isel(point=distance.argmin())\n",
    "    return float(closest.latitude), float(closest.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e5b088-daaa-4b6b-a900-75c8347abc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI-YEAR CONVLSTM WAVE FORECASTING\n",
      "======================================================================\n",
      "\n",
      "Found preprocessed_multiyear.pkl\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Load preprocessed data? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading preprocessed data from preprocessed_multiyear.pkl...\n",
      " Loaded preprocessed data\n",
      "Loaded: Train=(5825, 19, 21, 21, 6), Val=(1445, 19, 21, 21, 6)\n",
      "Loaded 2020: 1464 timesteps, shape=(1464, 21, 21)\n",
      "Loaded 2021: 1460 timesteps, shape=(1460, 21, 21)\n",
      "Loaded 2022: 1460 timesteps, shape=(1460, 21, 21)\n",
      "Loaded 2023: 1460 timesteps, shape=(1460, 21, 21)\n",
      "Loaded 2024: 1464 timesteps, shape=(1464, 21, 21)\n",
      "\n",
      " Concatenating datasets...\n",
      "\n",
      " Combined dataset:\n",
      "   Total timesteps: 7308\n",
      "   Time range: 2020-01-01T00:00:00.000000000 to 2024-12-31T18:00:00.000000000\n",
      "   Spatial shape: 21 Ã— 21\n",
      "======================================================================\n",
      "\n",
      " Building enhanced model...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 19, 21, 21, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 19, 21, 21, 128)   617984    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 19, 21, 21, 128)   512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 19, 21, 21, 128)   0         \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 19, 21, 21, 64)    442624    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 19, 21, 21, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 19, 21, 21, 64)    0         \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 19, 21, 21, 32)    110720    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 19, 21, 21, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv3d (Conv3D)             (None, 19, 21, 21, 3)     2595      \n",
      "                                                                 \n",
      " tf.__operators__.getitem (  (None, 21, 21, 3)         0         \n",
      " SlicingOpLambda)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1174819 (4.48 MB)\n",
      "Trainable params: 1174371 (4.48 MB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "_________________________________________________________________\n",
      "\n",
      " Training on 5825 samples...\n",
      "   Validation on 1445 samples from 2024\n",
      "Epoch 1/10\n",
      "365/365 [==============================] - 3354s 9s/step - loss: 0.0590 - mae: 0.1319 - val_loss: 0.0210 - val_mae: 0.0657 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "365/365 [==============================] - 3555s 10s/step - loss: 0.0173 - mae: 0.0705 - val_loss: 0.0142 - val_mae: 0.0550 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "365/365 [==============================] - 3503s 10s/step - loss: 0.0136 - mae: 0.0577 - val_loss: 0.0157 - val_mae: 0.0525 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "365/365 [==============================] - 3757s 10s/step - loss: 0.0125 - mae: 0.0533 - val_loss: 0.0140 - val_mae: 0.0569 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "365/365 [==============================] - 3789s 10s/step - loss: 0.0114 - mae: 0.0490 - val_loss: 0.0112 - val_mae: 0.0436 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "365/365 [==============================] - 3854s 11s/step - loss: 0.0105 - mae: 0.0454 - val_loss: 0.0093 - val_mae: 0.0336 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "365/365 [==============================] - 3543s 10s/step - loss: 0.0101 - mae: 0.0446 - val_loss: 0.0101 - val_mae: 0.0385 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.0098 - mae: 0.0437\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "365/365 [==============================] - 3484s 10s/step - loss: 0.0098 - mae: 0.0437 - val_loss: 0.0142 - val_mae: 0.0570 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "365/365 [==============================] - 4339s 12s/step - loss: 0.0085 - mae: 0.0358 - val_loss: 0.0085 - val_mae: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "365/365 [==============================] - 5268s 14s/step - loss: 0.0083 - mae: 0.0349 - val_loss: 0.0081 - val_mae: 0.0277 - lr: 1.0000e-04\n",
      "\n",
      "======================================================================\n",
      "MODEL EVALUATION\n",
      "======================================================================\n",
      "46/46 [==============================] - 226s 5s/step\n",
      "\n",
      "Overall Spatial Metrics:\n",
      "   MAE:  0.7277\n",
      "   RMSE: 4.4625\n",
      "\n",
      "Per-Variable Metrics:\n",
      "     shts: MAE=0.017, RMSE=0.045, RÂ²=0.993\n",
      "     mpts: MAE=0.086, RMSE=0.179, RÂ²=0.998\n",
      "     mdts: MAE=2.080, RMSE=7.727, RÂ²=0.989\n",
      "======================================================================\n",
      "\n",
      " Making forecast...\n",
      "\n",
      "======================================================================\n",
      "FORECAST at (7.00Â°N, 82.00Â°E)\n",
      "======================================================================\n",
      "Swell Height:    1.42 m\n",
      "Swell Period:    7.22 s\n",
      "Swell Direction: 69.9Â°\n",
      "======================================================================\n",
      "\n",
      "âœ“ Training complete! Model saved as 'best_multiyear_model.keras'\n",
      "\n",
      "ðŸ’¾ Saving model and metadata...\n",
      "âœ“ Saved: convlstm_final.keras + model_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# MAIN PIPELINE\n",
    "def main():\n",
    "    print(\"MULTI-YEAR CONVLSTM WAVE FORECASTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if preprocessed data exists\n",
    "    preprocessed_file = \"preprocessed_multiyear.pkl\"\n",
    "    \n",
    "    if Path(preprocessed_file).exists():\n",
    "        print(f\"\\nFound {preprocessed_file}\")\n",
    "        user_input = input(\"Load preprocessed data? (y/n): \")\n",
    "        \n",
    "        if user_input.lower() == 'y':\n",
    "            data = load_preprocessed_data(preprocessed_file)\n",
    "            X_train, y_train = data['X_train'], data['y_train']\n",
    "            X_val, y_val = data['X_val'], data['y_val']\n",
    "            scalers_X, scalers_y = data['scalers_X'], data['scalers_y']\n",
    "            \n",
    "            print(f\"Loaded: Train={X_train.shape}, Val={X_val.shape}\")\n",
    "            \n",
    "            # Still need to load ds for metadata\n",
    "            ds = load_multiyear_dataset(DATA_YEARS, DATA_DIR)\n",
    "            \n",
    "            skip_preprocessing = True\n",
    "        else:\n",
    "            skip_preprocessing = False\n",
    "    else:\n",
    "        skip_preprocessing = False\n",
    "    \n",
    "    if not skip_preprocessing:\n",
    "        # Step 1: Load all years\n",
    "        ds = load_multiyear_dataset(DATA_YEARS, DATA_DIR)\n",
    "        \n",
    "        # Step 2: Split by year\n",
    "        ds_train, ds_val = split_by_year(ds, TRAIN_YEARS, [VAL_YEAR])\n",
    "        \n",
    "        # Step 3: Create ocean mask (from combined data)\n",
    "        ocean_mask = create_ocean_mask(ds)\n",
    "        \n",
    "        # Step 4: Engineer features\n",
    "        print(\"\\nEngineering features...\")\n",
    "        X_train, y_train = engineer_frames(ds_train, ocean_mask)\n",
    "        X_val, y_val = engineer_frames(ds_val, ocean_mask)\n",
    "        \n",
    "        print(f\"   Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "        print(f\"   Val:   X={X_val.shape}, y={y_val.shape}\")\n",
    "        \n",
    "        # Step 5: Fit scalers on training data only\n",
    "        scalers_X, scalers_y = fit_scalers(X_train, y_train)\n",
    "        \n",
    "        # Step 6: Apply scalers\n",
    "        print(\"\\n Applying scalers...\")\n",
    "        X_train, y_train = apply_scalers(X_train, y_train, scalers_X, scalers_y)\n",
    "        X_val, y_val = apply_scalers(X_val, y_val, scalers_X, scalers_y)\n",
    "        \n",
    "        # Step 7: Create sequences\n",
    "        X_train, y_train = create_sequences(X_train, y_train, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "        X_val, y_val = create_sequences(X_val, y_val, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "        \n",
    "        print(f\"\\nâœ“ Final shapes:\")\n",
    "        print(f\"   Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "        print(f\"   Val:   X={X_val.shape}, y={y_val.shape}\")\n",
    "        \n",
    "        # Save for future runs\n",
    "        save_preprocessed_data(X_train, y_train, X_val, y_val, scalers_X, scalers_y, preprocessed_file)\n",
    "    \n",
    "    # Step 8: Build model\n",
    "    print(\"\\n Building enhanced model...\")\n",
    "    model = build_enhanced_model(X_train.shape[1:], y_train.shape[-1])\n",
    "    model.summary()\n",
    "    \n",
    "    # Step 9: Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=2,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Step 10: Train\n",
    "    print(f\"\\n Training on {len(X_train)} samples...\")\n",
    "    print(f\"   Validation on {len(X_val)} samples from {VAL_YEAR}\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Step 11: Evaluate\n",
    "    evaluate_model(model, X_val, y_val, scalers_y, TARGET_FEATURES)\n",
    "    \n",
    "    # Step 12: Forecast\n",
    "    print(\"\\n Making forecast...\")\n",
    "    buoy_lat, buoy_lon = find_nearest_ocean_point(ds, SPOT_LAT, SPOT_LON)\n",
    "    \n",
    "    # Get most recent sequence from validation set\n",
    "    last_seq = X_val[-1:] \n",
    "    forecast_scaled = model.predict(last_seq, verbose=0)[0]\n",
    "    \n",
    "    # Inverse transform\n",
    "    forecast = np.zeros_like(forecast_scaled)\n",
    "    for i in range(forecast_scaled.shape[-1]):\n",
    "        forecast[..., i] = scalers_y[i].inverse_transform(\n",
    "            forecast_scaled[..., i].reshape(-1, 1)\n",
    "        ).reshape(forecast_scaled[..., i].shape)\n",
    "    \n",
    "    # Extract buoy location\n",
    "    lats = ds.latitude.values\n",
    "    lons = ds.longitude.values\n",
    "    lat_idx = np.argmin(np.abs(lats - buoy_lat))\n",
    "    lon_idx = np.argmin(np.abs(lons - buoy_lon))\n",
    "    \n",
    "    buoy_forecast = forecast[lat_idx, lon_idx, :]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"FORECAST at ({buoy_lat:.2f}Â°N, {buoy_lon:.2f}Â°E)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Swell Height:    {buoy_forecast[0]:.2f} m\")\n",
    "    print(f\"Swell Period:    {buoy_forecast[1]:.2f} s\")\n",
    "    print(f\"Swell Direction: {buoy_forecast[2]:.1f}Â°\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nâœ“ Training complete! Model saved as 'best_multiyear_model.keras'\")\n",
    "    \n",
    "    \n",
    "    # ===============================================================\n",
    "    # SAVE MODEL + SCALERS + METADATA  (minimal patch)\n",
    "    # ===============================================================\n",
    "    print(\"\\nðŸ’¾ Saving model and metadata...\")\n",
    "\n",
    "    # Save trained ConvLSTM\n",
    "    model.save(\"convlstm_final.keras\")\n",
    "\n",
    "    # Prepare metadata dict\n",
    "    metadata = {\n",
    "        \"scalers_X\": scalers_X,\n",
    "        \"scalers_y\": scalers_y,\n",
    "        \"input_features\": INPUT_FEATURES,\n",
    "        \"target_features\": TARGET_FEATURES,\n",
    "        \"lookback_hours\": LOOKBACK_HOURS,\n",
    "        \"lookahead_hours\": LOOKAHEAD_HOURS,\n",
    "        \"latitude\": ds.latitude.values,\n",
    "        \"longitude\": ds.longitude.values,\n",
    "        \"training_years\": TRAIN_YEARS,\n",
    "        \"validation_year\": VAL_YEAR\n",
    "    }\n",
    "\n",
    "    # Save metadata to pickle\n",
    "    with open(\"model_metadata.pkl\", \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    print(\"âœ“ Saved: convlstm_final.keras + model_metadata.pkl\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a0f155-f92c-49fa-b7fa-45e79f14a174",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# After evaluate_model(...)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_multiyear_model_final.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ“ Final model (with best weights) saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_multiyear_model_final.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# After evaluate_model(...)\n",
    "model.save(\"best_multiyear_model_final.keras\")\n",
    "print(\"\\nâœ“ Final model (with best weights) saved as 'best_multiyear_model_final.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d872c7-2abb-4e83-a150-cee414a4d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy_metrics(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['mae'], label='Train MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "    plt.title('Model Accuracy (MAE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example use after model.fit():\n",
    "plot_accuracy_metrics(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8171938-1fd7-461c-94da-65ed272c198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Performance (Loss)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example use:\n",
    "# plot_performance(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985ef71-ddd1-4eac-bee3-c31d3f7f0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "\n",
    "start_time = time.time()\n",
    "process = psutil.Process()\n",
    "\n",
    "# Load or preprocess (simulate)\n",
    "_ = load_multiyear_dataset(DATA_YEARS, DATA_DIR)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "memory_used = process.memory_info().rss / (1024**3)\n",
    "print(f\"Elapsed Time: {elapsed:.2f} s, Memory Used: {memory_used:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b424e64-be89-41c2-b3bf-9cf897517556",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_true[...,0].flatten(), y_pred[...,0].flatten(), alpha=0.3)\n",
    "plt.xlabel(\"True Swell Height (m)\")\n",
    "plt.ylabel(\"Predicted Swell Height (m)\")\n",
    "plt.title(\"True vs Predicted Swell Height\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8b898-7551-4137-9065-ae5d26832678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33858587-faf0-4f66-8d62-95f4998cb611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
