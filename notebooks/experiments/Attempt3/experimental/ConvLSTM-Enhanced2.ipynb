{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4382119e-a348-4ba5-9f80-c96a23e678cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    ConvLSTM2D, Conv3D, BatchNormalization, Input, Dropout\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_YEARS = [2020, 2021, 2022, 2023, 2024, 2025]\n",
    "DATA_DIR = \"../\"\n",
    "SPOT_LAT, SPOT_LON = 6.8399, 81.8396\n",
    "\n",
    "LOOKBACK_HOURS = 19\n",
    "LOOKAHEAD_HOURS = 1\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "TRAIN_YEARS = [2020, 2021, 2022, 2023]\n",
    "VAL_YEAR = 2024\n",
    "\n",
    "INPUT_FEATURES = [\"u10\", \"v10\", \"msl\", \"shts\", \"mpts\", \"mdts\"]\n",
    "TARGET_FEATURES = [\"shts\", \"mpts\", \"mdts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c85bc9-1342-4d55-9df7-41d21b78416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiyear_dataset(years, data_dir=\".\"):   \n",
    "    datasets = []\n",
    "    for year in years:\n",
    "        filepath = Path(data_dir) / f\"surf_data_{year}.nc\"        \n",
    "        try:\n",
    "            ds = xr.open_dataset(filepath)\n",
    "            ds.attrs['year'] = year\n",
    "            datasets.append(ds)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {year}: {e}\")\n",
    "            continue\n",
    "    if not datasets:\n",
    "        raise ValueError(\"No valid datasets loaded!\")\n",
    "    \n",
    "    combined_ds = xr.concat(datasets, dim='valid_time')\n",
    "    combined_ds = combined_ds.sortby('valid_time')\n",
    "    print(f\"\\n Combined dataset:\")\n",
    "    print(f\"   Total timesteps: {len(combined_ds.valid_time)}\")\n",
    "    print(f\"   Time range: {combined_ds.valid_time.values[0]} to {combined_ds.valid_time.values[-1]}\")\n",
    "    print(f\"   Spatial shape: {len(combined_ds.latitude)} Ã— {len(combined_ds.longitude)}\")\n",
    "    \n",
    "    return combined_ds\n",
    "\n",
    "def split_by_year(ds, train_years, val_years):\n",
    "    train_mask = ds.valid_time.dt.year.isin(train_years)\n",
    "    val_mask = ds.valid_time.dt.year.isin(val_years)\n",
    "    ds_train = ds.isel(valid_time=train_mask)\n",
    "    ds_val = ds.isel(valid_time=val_mask)\n",
    "    \n",
    "    print(f\"\\n Data Split:\")\n",
    "    print(f\"   Training years: {train_years} â†’ {len(ds_train.valid_time)} samples\")\n",
    "    print(f\"   Validation years: {val_years} â†’ {len(ds_val.valid_time)} samples\")\n",
    "    print(f\"   Split ratio: {len(ds_train.valid_time)/(len(ds_train.valid_time)+len(ds_val.valid_time)):.1%} train\")\n",
    "    \n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2948a44-7ca1-4e64-bb97-ceeb43d97010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ocean_mask(ds, threshold=0.5):\n",
    "    shts_data = ds[\"shts\"].values\n",
    "    nan_ratio = np.isnan(shts_data).sum(axis=0) / shts_data.shape[0]\n",
    "    ocean_mask = nan_ratio < threshold\n",
    "    \n",
    "    valid_points = ocean_mask.sum()\n",
    "    print(f\"\\n Ocean Mask: {valid_points}/{ocean_mask.size} points ({100*valid_points/ocean_mask.size:.1f}%)\")\n",
    "    return ocean_mask\n",
    "\n",
    "\n",
    "def engineer_frames(ds, ocean_mask):\n",
    "    X = ds[INPUT_FEATURES].to_array(dim=\"channel\").transpose(\n",
    "        \"valid_time\", \"latitude\", \"longitude\", \"channel\"\n",
    "    ).values\n",
    "    \n",
    "    y = ds[TARGET_FEATURES].to_array(dim=\"channel\").transpose(\n",
    "        \"valid_time\", \"latitude\", \"longitude\", \"channel\"\n",
    "    ).values\n",
    "\n",
    "    mask_3d_X = np.broadcast_to(ocean_mask[..., np.newaxis], X.shape[1:])\n",
    "    mask_3d_y = np.broadcast_to(ocean_mask[..., np.newaxis], y.shape[1:])\n",
    "    \n",
    "    X[:, ~mask_3d_X] = 0.0\n",
    "    y[:, ~mask_3d_y] = 0.0\n",
    "    \n",
    "    for i in range(X.shape[-1]):\n",
    "        channel_mean = np.nanmean(X[..., i])\n",
    "        X[..., i] = np.nan_to_num(X[..., i], nan=channel_mean)\n",
    "    \n",
    "    for i in range(y.shape[-1]):\n",
    "        channel_mean = np.nanmean(y[..., i])\n",
    "        y[..., i] = np.nan_to_num(y[..., i], nan=channel_mean)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9c2ff8-474f-4422-b5a1-328dcc431976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_scalers(X_train, y_train):    \n",
    "    scalers_X = []\n",
    "    for i in range(X_train.shape[-1]):\n",
    "        scaler = StandardScaler()\n",
    "        channel = X_train[..., i].reshape(-1, 1)\n",
    "        scaler.fit(channel)\n",
    "        scalers_X.append(scaler)\n",
    "    \n",
    "    scalers_y = []\n",
    "    for i in range(y_train.shape[-1]):\n",
    "        scaler = StandardScaler()\n",
    "        channel = y_train[..., i].reshape(-1, 1)\n",
    "        scaler.fit(channel)\n",
    "        scalers_y.append(scaler)\n",
    "    \n",
    "    return scalers_X, scalers_y\n",
    "\n",
    "\n",
    "def apply_scalers(X, y, scalers_X, scalers_y):\n",
    "    X_scaled = np.zeros_like(X)\n",
    "    y_scaled = np.zeros_like(y)\n",
    "    \n",
    "    for i in range(X.shape[-1]):\n",
    "        channel = X[..., i].reshape(-1, 1)\n",
    "        X_scaled[..., i] = scalers_X[i].transform(channel).reshape(X[..., i].shape)\n",
    "    \n",
    "    for i in range(y.shape[-1]):\n",
    "        channel = y[..., i].reshape(-1, 1)\n",
    "        y_scaled[..., i] = scalers_y[i].transform(channel).reshape(y[..., i].shape)\n",
    "    \n",
    "    return X_scaled, y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4e84d-6666-46b6-a346-65420a5eefc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, lookback, lookahead):\n",
    "    total_sequences = len(X) - lookback - lookahead + 1\n",
    "    print(f\"\\n Creating {total_sequences} sequences...\")\n",
    "    \n",
    "    X_seq = np.zeros((total_sequences, lookback, *X.shape[1:]), dtype=np.float32)\n",
    "    y_seq = np.zeros((total_sequences, *y.shape[1:]), dtype=np.float32)\n",
    "    \n",
    "    for i in range(total_sequences):\n",
    "        X_seq[i] = X[i:i + lookback]\n",
    "        y_seq[i] = y[i + lookback + lookahead - 1]\n",
    "    \n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7293fd28-22c4-4630-aade-da6a50d55a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_data(X_train, y_train, X_val, y_val, scalers_X, scalers_y, filename=\"preprocessed_data.pkl\"):\n",
    "    data = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'scalers_X': scalers_X,\n",
    "        'scalers_y': scalers_y\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=4)\n",
    "\n",
    "def load_preprocessed_data(filename=\"preprocessed_data.pkl\"):\n",
    "    print(f\"\\n Loading preprocessed data from {filename}...\")\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(\" Loaded preprocessed data\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d3fbb-af61-45fc-b186-e264a02e6760",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d3fa47-cfdc-45f9-9f50-44dd90612128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enhanced_model(input_shape, output_channels):\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = ConvLSTM2D(\n",
    "        128, (3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    )(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = ConvLSTM2D(\n",
    "        64, (3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = ConvLSTM2D(\n",
    "        32, (3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=True\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv3D(\n",
    "        output_channels,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        padding='same',\n",
    "        activation='linear'\n",
    "    )(x)\n",
    "    \n",
    "    outputs = x[:, -1, :, :, :]\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67fea05-d538-4aab-8404-7ed0b73f6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, scalers_y, target_names):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    y_pred_scaled = model.predict(X_val, batch_size=32, verbose=1)\n",
    "    y_true = np.zeros_like(y_val)\n",
    "    y_pred = np.zeros_like(y_pred_scaled)\n",
    "    \n",
    "    for i in range(y_val.shape[-1]):\n",
    "        y_true[..., i] = scalers_y[i].inverse_transform(\n",
    "            y_val[..., i].reshape(-1, 1)\n",
    "        ).reshape(y_val[..., i].shape)\n",
    "        y_pred[..., i] = scalers_y[i].inverse_transform(\n",
    "            y_pred_scaled[..., i].reshape(-1, 1)\n",
    "        ).reshape(y_pred_scaled[..., i].shape)\n",
    "    \n",
    "    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "    rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))\n",
    "    \n",
    "    print(f\"\\nOverall Spatial Metrics:\")\n",
    "    print(f\"   MAE:  {mae:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "\n",
    "    for i, var in enumerate(target_names):\n",
    "        mae_var = mean_absolute_error(y_true[..., i].flatten(), y_pred[..., i].flatten())\n",
    "        rmse_var = np.sqrt(mean_squared_error(y_true[..., i].flatten(), y_pred[..., i].flatten()))\n",
    "        r2_var = r2_score(y_true[..., i].flatten(), y_pred[..., i].flatten())\n",
    "        \n",
    "        print(f\"   {var:>6s}: MAE={mae_var:.3f}, RMSE={rmse_var:.3f}, RÂ²={r2_var:.3f}\")\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765d6aa-5976-4274-b5a8-2ee66c4d4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_ocean_point(ds, lat, lon):\n",
    "    nearest = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "    \n",
    "    if not np.isnan(nearest[\"shts\"].isel(valid_time=0)):\n",
    "        return float(nearest.latitude), float(nearest.longitude)\n",
    "    \n",
    "    shts_data = ds[\"shts\"].isel(valid_time=0).stack(\n",
    "        point=(\"latitude\", \"longitude\")\n",
    "    ).dropna(\"point\")\n",
    "    \n",
    "    R = 6371\n",
    "    lat1, lon1 = np.radians(lat), np.radians(lon)\n",
    "    lat2 = np.radians(shts_data.latitude)\n",
    "    lon2 = np.radians(shts_data.longitude)\n",
    "    \n",
    "    dlat = (lat2 - lat1) / 2\n",
    "    dlon = (lon2 - lon1) / 2\n",
    "    \n",
    "    a = np.sin(dlat)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon)**2\n",
    "    distance = 2 * R * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    closest = shts_data.isel(point=distance.argmin())\n",
    "    return float(closest.latitude), float(closest.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e5b088-daaa-4b6b-a900-75c8347abc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI-YEAR CONVLSTM WAVE FORECASTING\n",
      "======================================================================\n",
      "\n",
      "Found preprocessed_multiyear.pkl\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Load preprocessed data? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading preprocessed data from preprocessed_multiyear.pkl...\n",
      " Loaded preprocessed data\n",
      "Loaded: Train=(5825, 19, 21, 21, 6), Val=(1445, 19, 21, 21, 6)\n",
      "Loaded 2020: 1464 timesteps, shape=(1464, 21, 21)\n",
      "Loaded 2021: 1460 timesteps, shape=(1460, 21, 21)\n",
      "Loaded 2022: 1460 timesteps, shape=(1460, 21, 21)\n",
      "Loaded 2023: 1460 timesteps, shape=(1460, 21, 21)\n",
      "Loaded 2024: 1464 timesteps, shape=(1464, 21, 21)\n",
      "\n",
      " Concatenating datasets...\n",
      "\n",
      " Combined dataset:\n",
      "   Total timesteps: 7308\n",
      "   Time range: 2020-01-01T00:00:00.000000000 to 2024-12-31T18:00:00.000000000\n",
      "   Spatial shape: 21 Ã— 21\n",
      "======================================================================\n",
      "\n",
      " Building enhanced model...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 19, 21, 21, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 19, 21, 21, 128)   617984    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 19, 21, 21, 128)   512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 19, 21, 21, 128)   0         \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 19, 21, 21, 64)    442624    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 19, 21, 21, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 19, 21, 21, 64)    0         \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 19, 21, 21, 32)    110720    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 19, 21, 21, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv3d (Conv3D)             (None, 19, 21, 21, 3)     2595      \n",
      "                                                                 \n",
      " tf.__operators__.getitem (  (None, 21, 21, 3)         0         \n",
      " SlicingOpLambda)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1174819 (4.48 MB)\n",
      "Trainable params: 1174371 (4.48 MB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "_________________________________________________________________\n",
      "\n",
      " Training on 5825 samples...\n",
      "   Validation on 1445 samples from 2024\n",
      "Epoch 1/10\n",
      "365/365 [==============================] - 3354s 9s/step - loss: 0.0590 - mae: 0.1319 - val_loss: 0.0210 - val_mae: 0.0657 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "365/365 [==============================] - 3555s 10s/step - loss: 0.0173 - mae: 0.0705 - val_loss: 0.0142 - val_mae: 0.0550 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "365/365 [==============================] - 3503s 10s/step - loss: 0.0136 - mae: 0.0577 - val_loss: 0.0157 - val_mae: 0.0525 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "365/365 [==============================] - 3757s 10s/step - loss: 0.0125 - mae: 0.0533 - val_loss: 0.0140 - val_mae: 0.0569 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "365/365 [==============================] - 3789s 10s/step - loss: 0.0114 - mae: 0.0490 - val_loss: 0.0112 - val_mae: 0.0436 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "365/365 [==============================] - 3854s 11s/step - loss: 0.0105 - mae: 0.0454 - val_loss: 0.0093 - val_mae: 0.0336 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "365/365 [==============================] - 3543s 10s/step - loss: 0.0101 - mae: 0.0446 - val_loss: 0.0101 - val_mae: 0.0385 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.0098 - mae: 0.0437\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "365/365 [==============================] - 3484s 10s/step - loss: 0.0098 - mae: 0.0437 - val_loss: 0.0142 - val_mae: 0.0570 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "365/365 [==============================] - 4339s 12s/step - loss: 0.0085 - mae: 0.0358 - val_loss: 0.0085 - val_mae: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "365/365 [==============================] - 5268s 14s/step - loss: 0.0083 - mae: 0.0349 - val_loss: 0.0081 - val_mae: 0.0277 - lr: 1.0000e-04\n",
      "\n",
      "======================================================================\n",
      "MODEL EVALUATION\n",
      "======================================================================\n",
      "46/46 [==============================] - 226s 5s/step\n",
      "\n",
      "Overall Spatial Metrics:\n",
      "   MAE:  0.7277\n",
      "   RMSE: 4.4625\n",
      "\n",
      "Per-Variable Metrics:\n",
      "     shts: MAE=0.017, RMSE=0.045, RÂ²=0.993\n",
      "     mpts: MAE=0.086, RMSE=0.179, RÂ²=0.998\n",
      "     mdts: MAE=2.080, RMSE=7.727, RÂ²=0.989\n",
      "======================================================================\n",
      "\n",
      " Making forecast...\n",
      "\n",
      "======================================================================\n",
      "FORECAST at (7.00Â°N, 82.00Â°E)\n",
      "======================================================================\n",
      "Swell Height:    1.42 m\n",
      "Swell Period:    7.22 s\n",
      "Swell Direction: 69.9Â°\n",
      "======================================================================\n",
      "\n",
      "âœ“ Training complete! Model saved as 'best_multiyear_model.keras'\n",
      "\n",
      "ðŸ’¾ Saving model and metadata...\n",
      "âœ“ Saved: convlstm_final.keras + model_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    preprocessed_file = \"preprocessed_multiyear.pkl\"\n",
    "    \n",
    "    if Path(preprocessed_file).exists():\n",
    "        print(f\"\\nFound {preprocessed_file}\")\n",
    "        user_input = input(\"Load preprocessed data? (y/n): \")\n",
    "        \n",
    "        if user_input.lower() == 'y':\n",
    "            data = load_preprocessed_data(preprocessed_file)\n",
    "            X_train, y_train = data['X_train'], data['y_train']\n",
    "            X_val, y_val = data['X_val'], data['y_val']\n",
    "            scalers_X, scalers_y = data['scalers_X'], data['scalers_y']\n",
    "            print(f\"Loaded: Train={X_train.shape}, Val={X_val.shape}\")\n",
    "            ds = load_multiyear_dataset(DATA_YEARS, DATA_DIR)\n",
    "    \n",
    "            skip_preprocessing = True\n",
    "        else:\n",
    "            skip_preprocessing = False\n",
    "    else:\n",
    "        skip_preprocessing = False\n",
    "    \n",
    "    if not skip_preprocessing:\n",
    "        ds = load_multiyear_dataset(DATA_YEARS, DATA_DIR)\n",
    "        ds_train, ds_val = split_by_year(ds, TRAIN_YEARS, [VAL_YEAR])\n",
    "        ocean_mask = create_ocean_mask(ds)\n",
    "        \n",
    "        print(\"\\nEngineering features...\")\n",
    "        X_train, y_train = engineer_frames(ds_train, ocean_mask)\n",
    "        X_val, y_val = engineer_frames(ds_val, ocean_mask)\n",
    "        \n",
    "        print(f\"   Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "        print(f\"   Val:   X={X_val.shape}, y={y_val.shape}\")\n",
    "        \n",
    "        scalers_X, scalers_y = fit_scalers(X_train, y_train)\n",
    "        X_train, y_train = apply_scalers(X_train, y_train, scalers_X, scalers_y)\n",
    "        X_val, y_val = apply_scalers(X_val, y_val, scalers_X, scalers_y)\n",
    "\n",
    "        X_train, y_train = create_sequences(X_train, y_train, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "        X_val, y_val = create_sequences(X_val, y_val, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "        save_preprocessed_data(X_train, y_train, X_val, y_val, scalers_X, scalers_y, preprocessed_file)\n",
    "    \n",
    "    model = build_enhanced_model(X_train.shape[1:], y_train.shape[-1])\n",
    "    model.summary()\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=2,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n Training on {len(X_train)} samples...\")\n",
    "    print(f\"   Validation on {len(X_val)} samples from {VAL_YEAR}\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    evaluate_model(model, X_val, y_val, scalers_y, TARGET_FEATURES)\n",
    "    buoy_lat, buoy_lon = find_nearest_ocean_point(ds, SPOT_LAT, SPOT_LON)\n",
    "    last_seq = X_val[-1:] \n",
    "    forecast_scaled = model.predict(last_seq, verbose=0)[0]\n",
    "    forecast = np.zeros_like(forecast_scaled)\n",
    "    for i in range(forecast_scaled.shape[-1]):\n",
    "        forecast[..., i] = scalers_y[i].inverse_transform(\n",
    "            forecast_scaled[..., i].reshape(-1, 1)\n",
    "        ).reshape(forecast_scaled[..., i].shape)\n",
    "    lats = ds.latitude.values\n",
    "    lons = ds.longitude.values\n",
    "    lat_idx = np.argmin(np.abs(lats - buoy_lat))\n",
    "    lon_idx = np.argmin(np.abs(lons - buoy_lon))\n",
    "    \n",
    "    buoy_forecast = forecast[lat_idx, lon_idx, :]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"FORECAST at ({buoy_lat:.2f}Â°N, {buoy_lon:.2f}Â°E)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Swell Height:    {buoy_forecast[0]:.2f} m\")\n",
    "    print(f\"Swell Period:    {buoy_forecast[1]:.2f} s\")\n",
    "    print(f\"Swell Direction: {buoy_forecast[2]:.1f}Â°\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"\\n Saving model and metadata...\")\n",
    "    model.save(\"convlstm_final.keras\")\n",
    "\n",
    "    metadata = {\n",
    "        \"scalers_X\": scalers_X,\n",
    "        \"scalers_y\": scalers_y,\n",
    "        \"input_features\": INPUT_FEATURES,\n",
    "        \"target_features\": TARGET_FEATURES,\n",
    "        \"lookback_hours\": LOOKBACK_HOURS,\n",
    "        \"lookahead_hours\": LOOKAHEAD_HOURS,\n",
    "        \"latitude\": ds.latitude.values,\n",
    "        \"longitude\": ds.longitude.values,\n",
    "        \"training_years\": TRAIN_YEARS,\n",
    "        \"validation_year\": VAL_YEAR\n",
    "    }\n",
    "    with open(\"model_metadata.pkl\", \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    print(\" Saved: convlstm_final.keras + model_metadata.pkl\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33858587-faf0-4f66-8d62-95f4998cb611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
