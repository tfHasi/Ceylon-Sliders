{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a748080-c39a-4469-a2cb-2e201b53d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Flatten, Dense, BatchNormalization, Dropout, PReLU\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import sys\n",
    "\n",
    "# Python 3.9 compatibility fix\n",
    "if sys.version_info >= (3, 10):\n",
    "    from typing import TypeAlias\n",
    "else:\n",
    "    from typing_extensions import TypeAlias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9f19aa-8801-48bd-8768-02c805195f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "DATA_PATH = \"../surf_data_2024.nc\"\n",
    "SPOT_LAT, SPOT_LON = 6.8399, 81.8396  # Arugam Bay\n",
    "LOOKBACK_HOURS = 28\n",
    "LOOKAHEAD_HOURS = 12\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "INPUT_FEATURES = [\"u10\", \"v10\", \"msl\", \"shts\", \"mpts\", \"mdts\"]\n",
    "TARGET_VARS = [\"shts\", \"mpts\", \"mdts_sin\", \"mdts_cos\", \"wind_speed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb0174e-df64-477c-ae64-1eb9af887481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED NaN DIAGNOSTICS\n",
    "def diagnose_nans(ds, features):\n",
    "    \"\"\"Detailed NaN analysis before preprocessing.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NaN DIAGNOSTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for var in features:\n",
    "        data = ds[var].values\n",
    "        nan_count = np.isnan(data).sum()\n",
    "        nan_pct = 100 * nan_count / data.size\n",
    "        \n",
    "        # Check if NaNs are in specific locations\n",
    "        nan_mask = np.isnan(data)\n",
    "        if nan_count > 0:\n",
    "            # Spatial distribution\n",
    "            spatial_nans = nan_mask.any(axis=0)  # Any NaN across time\n",
    "            temporal_nans = nan_mask.any(axis=(1, 2))  # Any NaN across space\n",
    "            \n",
    "            print(f\"\\n{var}:\")\n",
    "            print(f\"  Total NaNs: {nan_count:,} ({nan_pct:.2f}%)\")\n",
    "            print(f\"  Affected spatial points: {spatial_nans.sum()} / {spatial_nans.size}\")\n",
    "            print(f\"  Affected timesteps: {temporal_nans.sum()} / {len(temporal_nans)}\")\n",
    "        else:\n",
    "            print(f\"\\n{var}: ‚úì No NaNs\")\n",
    "    \n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b4767d-f2bc-48d9-b1ae-83b9dc870400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# SOLUTION 1: OCEAN MASK (Recommended for Spatial Data)\n",
    "# ======================================================================\n",
    "def create_ocean_mask(ds, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Create a mask to exclude land points.\n",
    "    Points with >50% NaN values across time are considered land.\n",
    "    \"\"\"\n",
    "    # Use wave height as indicator (always NaN over land)\n",
    "    shts_data = ds[\"shts\"].values\n",
    "    nan_ratio = np.isnan(shts_data).sum(axis=0) / shts_data.shape[0]\n",
    "    ocean_mask = nan_ratio < threshold\n",
    "    \n",
    "    valid_points = ocean_mask.sum()\n",
    "    total_points = ocean_mask.size\n",
    "    \n",
    "    print(f\"\\nüåä Ocean Mask Created:\")\n",
    "    print(f\"   Valid ocean points: {valid_points} / {total_points} ({100*valid_points/total_points:.1f}%)\")\n",
    "    \n",
    "    return ocean_mask\n",
    "\n",
    "\n",
    "def apply_ocean_mask(X, ocean_mask):\n",
    "    \"\"\"Apply ocean mask to spatial data, setting land points to 0.\"\"\"\n",
    "    X_masked = X.copy()\n",
    "    # Broadcast mask to all timesteps and channels\n",
    "    mask_3d = np.broadcast_to(ocean_mask[..., np.newaxis], X.shape)\n",
    "    X_masked[~mask_3d] = 0.0\n",
    "    return X_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca24509-8ddc-4dc4-8333-5488ef6a2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# SOLUTION 2: TEMPORAL INTERPOLATION (COMMENTED OUT)\n",
    "# ======================================================================\n",
    "# def interpolate_temporal_gaps(ds, features, method='linear', limit=2):\n",
    "#     \"\"\"\n",
    "#     Fill small temporal gaps using interpolation.\n",
    "#     Only fills gaps up to 'limit' consecutive NaNs.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n‚è±Ô∏è  Interpolating temporal gaps (method={method}, max_gap={limit})...\")\n",
    "#     \n",
    "#     ds_interp = ds.copy()\n",
    "#     for var in features:\n",
    "#         original_nans = np.isnan(ds[var].values).sum()\n",
    "#         \n",
    "#         # Interpolate along time dimension\n",
    "#         ds_interp[var] = ds[var].interpolate_na(\n",
    "#             dim='valid_time', \n",
    "#             method=method,\n",
    "#             limit=limit,\n",
    "#             fill_value='extrapolate'\n",
    "#         )\n",
    "#         \n",
    "#         remaining_nans = np.isnan(ds_interp[var].values).sum()\n",
    "#         filled = original_nans - remaining_nans\n",
    "#         \n",
    "#         if filled > 0:\n",
    "#             print(f\"   {var}: Filled {filled:,} NaNs ({original_nans:,} ‚Üí {remaining_nans:,})\")\n",
    "#     \n",
    "#     return ds_interp\n",
    "# Simple function that just returns the original dataset\n",
    "def interpolate_temporal_gaps(ds, features, method='linear', limit=2):\n",
    "    print(\"‚è≠Ô∏è  Skipping temporal interpolation...\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b49efa7-99d5-4921-b4b3-d002cadf8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# SOLUTION 3: SMART NaN REMOVAL\n",
    "# ======================================================================\n",
    "def remove_corrupted_timesteps(X, y, max_nan_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Remove timesteps where NaN ratio exceeds threshold.\n",
    "    More conservative than filling with zeros.\n",
    "    \"\"\"\n",
    "    nan_ratio_per_time = np.isnan(X).sum(axis=(1, 2, 3)) / (X.shape[1] * X.shape[2] * X.shape[3])\n",
    "    valid_mask = nan_ratio_per_time < max_nan_ratio\n",
    "    \n",
    "    removed = len(X) - valid_mask.sum()\n",
    "    if removed > 0:\n",
    "        print(f\"\\nüóëÔ∏è  Removed {removed} corrupted timesteps (>{max_nan_ratio*100:.0f}% NaNs)\")\n",
    "    \n",
    "    return X[valid_mask], y[valid_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbbbc39-bb08-4a4b-b414-ae6195c5a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# IMPROVED PREPROCESSING\n",
    "# ======================================================================\n",
    "def preprocess_data_v2(X, y, ocean_mask=None):\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing with proper NaN handling.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREPROCESSING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Apply ocean mask if provided\n",
    "    if ocean_mask is not None:\n",
    "        X = apply_ocean_mask(X, ocean_mask)\n",
    "        print(\"‚úì Ocean mask applied (land points ‚Üí 0)\")\n",
    "    \n",
    "    # Step 2: Check remaining NaNs\n",
    "    nan_count_X = np.isnan(X).sum()\n",
    "    nan_count_y = np.isnan(y).sum()\n",
    "    \n",
    "    print(f\"\\nRemaining NaNs:\")\n",
    "    print(f\"   Input (X): {nan_count_X:,} ({100*nan_count_X/X.size:.3f}%)\")\n",
    "    print(f\"   Target (y): {nan_count_y:,} ({100*nan_count_y/y.size:.3f}%)\")\n",
    "    \n",
    "    # Step 3: Remove rows with NaN targets (critical!)\n",
    "    if nan_count_y > 0:\n",
    "        valid_mask = ~np.isnan(y).any(axis=1)\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        print(f\"‚úì Removed {(~valid_mask).sum()} samples with NaN targets\")\n",
    "    \n",
    "    # Step 4: Handle remaining spatial NaNs (e.g., edge effects)\n",
    "    # Fill with spatial mean (better than 0 for model learning)\n",
    "    if nan_count_X > 0:\n",
    "        for i in range(X.shape[-1]):\n",
    "            channel = X[..., i]\n",
    "            channel_mean = np.nanmean(channel)\n",
    "            X[..., i] = np.nan_to_num(channel, nan=channel_mean)\n",
    "        print(f\"‚úì Filled remaining input NaNs with channel means\")\n",
    "    \n",
    "    # Step 5: Scale data\n",
    "    X_scaled = np.zeros_like(X)\n",
    "    scalers_X = []\n",
    "    \n",
    "    for i in range(X.shape[-1]):\n",
    "        scaler = StandardScaler()\n",
    "        channel = X[..., i].reshape(-1, 1)\n",
    "        X_scaled[..., i] = scaler.fit_transform(channel).reshape(X[..., i].shape)\n",
    "        scalers_X.append(scaler)\n",
    "    \n",
    "    scaler_y = StandardScaler()\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "    \n",
    "    print(f\"\\n‚úì Scaled data | X: {X_scaled.shape}, y: {y_scaled.shape}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return X_scaled, y_scaled, scalers_X, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8296e06-ad88-4bf6-9fc4-6ae37938462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# IMPROVED FEATURE ENGINEERING\n",
    "# ======================================================================\n",
    "def engineer_features_v2(ds, buoy_lat, buoy_lon):\n",
    "    \"\"\"Feature engineering with NaN tracking.\"\"\"\n",
    "    # Spatial input\n",
    "    X = ds[INPUT_FEATURES].to_array(dim=\"channel\").transpose(\n",
    "        \"valid_time\", \"latitude\", \"longitude\", \"channel\"\n",
    "    ).values\n",
    "    \n",
    "    # Target point\n",
    "    buoy_data = ds.sel(latitude=buoy_lat, longitude=buoy_lon)\n",
    "    \n",
    "    wind_speed = np.sqrt(buoy_data[\"u10\"]**2 + buoy_data[\"v10\"]**2).values\n",
    "    mdts_rad = np.deg2rad(buoy_data[\"mdts\"].values)\n",
    "    \n",
    "    y = np.column_stack([\n",
    "        buoy_data[\"shts\"].values,\n",
    "        buoy_data[\"mpts\"].values,\n",
    "        np.sin(mdts_rad),\n",
    "        np.cos(mdts_rad),\n",
    "        wind_speed\n",
    "    ])\n",
    "    \n",
    "    # Diagnostics\n",
    "    print(f\"\\nüìä Feature Extraction:\")\n",
    "    print(f\"   Input shape: {X.shape}\")\n",
    "    print(f\"   Target shape: {y.shape}\")\n",
    "    print(f\"   Input NaNs: {np.isnan(X).sum():,}\")\n",
    "    print(f\"   Target NaNs: {np.isnan(y).sum():,}\")\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91dbdcf-6a47-431e-8a9b-bd573674e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# FIND NEAREST OCEAN POINT (Same as before)\n",
    "# ======================================================================\n",
    "def find_nearest_ocean_point(ds, lat, lon):\n",
    "    \"\"\"Find nearest valid ocean point using Haversine distance.\"\"\"\n",
    "    nearest = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "    \n",
    "    if not np.isnan(nearest[\"shts\"].isel(valid_time=0)):\n",
    "        print(\"‚úì Found valid offshore point\")\n",
    "        return float(nearest.latitude), float(nearest.longitude)\n",
    "    \n",
    "    print(\"‚ö† Nearest point on land. Searching for closest ocean point...\")\n",
    "    shts_data = ds[\"shts\"].isel(valid_time=0).stack(\n",
    "        point=(\"latitude\", \"longitude\")\n",
    "    ).dropna(\"point\")\n",
    "    \n",
    "    R = 6371\n",
    "    lat1, lon1 = np.radians(lat), np.radians(lon)\n",
    "    lat2 = np.radians(shts_data.latitude)\n",
    "    lon2 = np.radians(shts_data.longitude)\n",
    "    \n",
    "    dlat = (lat2 - lat1) / 2\n",
    "    dlon = (lon2 - lon1) / 2\n",
    "    \n",
    "    a = np.sin(dlat)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon)**2\n",
    "    distance = 2 * R * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    closest = shts_data.isel(point=distance.argmin())\n",
    "    return float(closest.latitude), float(closest.longitude)\n",
    "\n",
    "\n",
    "def create_sequences(X, y, lookback, lookahead):\n",
    "    \"\"\"Generate sliding window sequences.\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - lookback - lookahead + 1):\n",
    "        X_seq.append(X[i:i + lookback])\n",
    "        y_seq.append(y[i + lookback + lookahead - 1])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "def build_model_v2(input_shape, output_dim):\n",
    "    \"\"\"Stacked ConvLSTM architecture.\"\"\"\n",
    "    model = Sequential([\n",
    "        ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        ConvLSTM2D(32, (3, 3), padding='same', return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128),\n",
    "        PReLU(),\n",
    "        Dropout(0.2),\n",
    "        Dense(64),\n",
    "        PReLU(),\n",
    "        Dropout(0.3),\n",
    "        Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.AdamW(learning_rate=5e-4, weight_decay=1e-5),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_val, y_val, scaler_y, target_names):\n",
    "    \"\"\"Evaluation metrics.\"\"\"\n",
    "    y_pred_scaled = model.predict(X_val, verbose=0)\n",
    "    \n",
    "    y_true = scaler_y.inverse_transform(y_val)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    mae_overall = mean_absolute_error(y_true, y_pred)\n",
    "    rmse_overall = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2_overall = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATION METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Overall | MAE={mae_overall:.4f}, RMSE={rmse_overall:.4f}, R¬≤={r2_overall:.4f}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for i, var in enumerate(target_names):\n",
    "        mae_i = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        rmse_i = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))\n",
    "        r2_i = r2_score(y_true[:, i], y_pred[:, i])\n",
    "        print(f\"{var:>12s} | MAE={mae_i:.3f}, RMSE={rmse_i:.3f}, R¬≤={r2_i:.3f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a24bf8a-8154-40e6-9b3a-a37ddbe12b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä IMPROVED CONVLSTM PIPELINE WITH NaN HANDLING\n",
      "======================================================================\n",
      "\n",
      "üìÇ Loading dataset...\n",
      "\n",
      "======================================================================\n",
      "NaN DIAGNOSTICS\n",
      "======================================================================\n",
      "\n",
      "u10: ‚úì No NaNs\n",
      "\n",
      "v10: ‚úì No NaNs\n",
      "\n",
      "msl: ‚úì No NaNs\n",
      "\n",
      "shts:\n",
      "  Total NaNs: 509,472 (78.91%)\n",
      "  Affected spatial points: 348 / 441\n",
      "  Affected timesteps: 1464 / 1464\n",
      "\n",
      "mpts:\n",
      "  Total NaNs: 509,472 (78.91%)\n",
      "  Affected spatial points: 348 / 441\n",
      "  Affected timesteps: 1464 / 1464\n",
      "\n",
      "mdts:\n",
      "  Total NaNs: 509,472 (78.91%)\n",
      "  Affected spatial points: 348 / 441\n",
      "  Affected timesteps: 1464 / 1464\n",
      "======================================================================\n",
      "‚è≠Ô∏è  Skipping temporal interpolation...\n",
      "‚ö† Nearest point on land. Searching for closest ocean point...\n",
      "üìç Virtual buoy: (7.00¬∞N, 82.00¬∞E)\n",
      "\n",
      "üåä Ocean Mask Created:\n",
      "   Valid ocean points: 93 / 441 (21.1%)\n",
      "\n",
      "üìä Feature Extraction:\n",
      "   Input shape: (1464, 21, 21, 6)\n",
      "   Target shape: (1464, 5)\n",
      "   Input NaNs: 1,528,416\n",
      "   Target NaNs: 0\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING\n",
      "======================================================================\n",
      "‚úì Ocean mask applied (land points ‚Üí 0)\n",
      "\n",
      "Remaining NaNs:\n",
      "   Input (X): 0 (0.000%)\n",
      "   Target (y): 0 (0.000%)\n",
      "\n",
      "‚úì Scaled data | X: (1464, 21, 21, 6), y: (1464, 5)\n",
      "======================================================================\n",
      "\n",
      "üîÑ Creating sequences (lookback=28, lookahead=12)...\n",
      "   Sequence shape: X=(1425, 28, 21, 21, 6), y=(1425, 5)\n",
      "\n",
      "üèóÔ∏è Building model...\n",
      "\n",
      "üöÄ Training for up to 15 epochs...\n",
      "Epoch 1/15\n",
      "36/36 [==============================] - 224s 6s/step - loss: 1.3640 - mae: 0.8777 - val_loss: 1.0509 - val_mae: 0.8159 - lr: 5.0000e-04\n",
      "Epoch 2/15\n",
      "36/36 [==============================] - 229s 6s/step - loss: 0.8047 - mae: 0.7112 - val_loss: 0.9701 - val_mae: 0.7809 - lr: 5.0000e-04\n",
      "Epoch 3/15\n",
      "36/36 [==============================] - 227s 6s/step - loss: 0.7341 - mae: 0.6718 - val_loss: 1.0427 - val_mae: 0.8028 - lr: 5.0000e-04\n",
      "Epoch 4/15\n",
      "36/36 [==============================] - 223s 6s/step - loss: 0.7030 - mae: 0.6517 - val_loss: 0.9401 - val_mae: 0.7690 - lr: 5.0000e-04\n",
      "Epoch 5/15\n",
      "36/36 [==============================] - 218s 6s/step - loss: 0.6705 - mae: 0.6354 - val_loss: 0.9976 - val_mae: 0.7867 - lr: 5.0000e-04\n",
      "Epoch 6/15\n",
      " 6/36 [====>.........................] - ETA: 3:02 - loss: 0.6418 - mae: 0.6017"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m\n\u001b[0;32m     40\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     41\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     42\u001b[0m     ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m ]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müöÄ Training for up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     53\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m     56\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(VALIDATION_SPLIT \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_seq))\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mC:\\Hasi\\WorkShit\\Ceylon-Surfers\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# MAIN EXECUTION\n",
    "# ======================================================================\n",
    "\n",
    "print(\"üåä IMPROVED CONVLSTM PIPELINE WITH NaN HANDLING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "print(\"\\nüìÇ Loading dataset...\")\n",
    "ds = xr.open_dataset(DATA_PATH)\n",
    "\n",
    "# Diagnose NaNs BEFORE processing\n",
    "diagnose_nans(ds, INPUT_FEATURES)\n",
    "\n",
    "# STRATEGY 1: Interpolate small gaps\n",
    "ds = interpolate_temporal_gaps(ds, INPUT_FEATURES, method='linear', limit=2)\n",
    "\n",
    "# Find buoy location\n",
    "buoy_lat, buoy_lon = find_nearest_ocean_point(ds, SPOT_LAT, SPOT_LON)\n",
    "print(f\"üìç Virtual buoy: ({buoy_lat:.2f}¬∞N, {buoy_lon:.2f}¬∞E)\")\n",
    "\n",
    "# STRATEGY 2: Create ocean mask\n",
    "ocean_mask = create_ocean_mask(ds)\n",
    "\n",
    "# Feature engineering\n",
    "X, y = engineer_features_v2(ds, buoy_lat, buoy_lon)\n",
    "\n",
    "# STRATEGY 3: Improved preprocessing\n",
    "X_scaled, y_scaled, scalers_X, scaler_y = preprocess_data_v2(X, y, ocean_mask)\n",
    "\n",
    "# Create sequences\n",
    "print(f\"\\nüîÑ Creating sequences (lookback={LOOKBACK_HOURS}, lookahead={LOOKAHEAD_HOURS})...\")\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "print(f\"   Sequence shape: X={X_seq.shape}, y={y_seq.shape}\")\n",
    "\n",
    "# Build and train\n",
    "print(\"\\nüèóÔ∏è Building model...\")\n",
    "model = build_model_v2(X_seq.shape[1:], y_seq.shape[1])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nüöÄ Training for up to {EPOCHS} epochs...\")\n",
    "history = model.fit(\n",
    "    X_seq, y_seq,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "val_size = int(VALIDATION_SPLIT * len(X_seq))\n",
    "evaluate_model(model, X_seq[-val_size:], y_seq[-val_size:], scaler_y, TARGET_VARS)\n",
    "\n",
    "# Forecast\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÆ FORECAST (Next 12 Hours)\")\n",
    "print(\"=\"*70)\n",
    "last_seq = np.expand_dims(X_scaled[-LOOKBACK_HOURS:], axis=0)\n",
    "forecast_scaled = model.predict(last_seq, verbose=0)\n",
    "forecast = scaler_y.inverse_transform(forecast_scaled)[0]\n",
    "\n",
    "direction_deg = np.rad2deg(np.arctan2(forecast[2], forecast[3])) % 360\n",
    "\n",
    "print(f\"Swell Height:    {forecast[0]:.2f} m\")\n",
    "print(f\"Swell Period:    {forecast[1]:.2f} s\")\n",
    "print(f\"Swell Direction: {direction_deg:.1f}¬∞\")\n",
    "print(f\"Wind Speed:      {forecast[4]:.2f} m/s ({forecast[4]*3.6:.1f} km/h)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
