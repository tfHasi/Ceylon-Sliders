{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be3e09-e941-4b83-b065-cdcec2b217e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiyear ConvLSTM Wave Forecasting\n",
    "# Jupyter-notebook friendly (cell-separated) refactor\n",
    "# - Minimal, readable, modular\n",
    "# - Keeps full functionality from original script\n",
    "# - Designed to run in notebook cells (or as a linear script)\n",
    "\n",
    "# %%\n",
    "\"\"\"\n",
    "CONFIG / IMPORTS\n",
    "\"\"\"\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, ConvLSTM2D, Conv3D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# config\n",
    "DATA_YEARS = [2020, 2021, 2022, 2023, 2024]\n",
    "DATA_DIR = \"../\"  # where surf_data_YYYY.nc live\n",
    "TRAIN_YEARS = [2020, 2021, 2022, 2023]\n",
    "VAL_YEAR = 2024\n",
    "LOOKBACK_HOURS = 19\n",
    "LOOKAHEAD_HOURS = 1\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "INPUT_FEATURES = [\"u10\", \"v10\", \"msl\", \"shts\", \"mpts\", \"mdts\"]\n",
    "TARGET_FEATURES = [\"shts\", \"mpts\", \"mdts\"]\n",
    "PREPROCESSED_FILE = \"preprocessed_multiyear.pkl\"\n",
    "MODEL_SAVE_PATH = \"best_multiyear_model.keras\"\n",
    "SPOT_LAT, SPOT_LON = 6.8399, 81.8396\n",
    "\n",
    "# Toggle to load preprocessed data (useful in notebooks)\n",
    "LOAD_PREPROCESSED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0836959-a154-4880-bbc3-c15a1f8ac4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UTILITY: Data loading and splitting\n",
    "\"\"\"\n",
    "def load_multiyear_dataset(years, data_dir=\".\"):\n",
    "    files = [Path(data_dir) / f\"surf_data_{y}.nc\" for y in years]\n",
    "    datasets = []\n",
    "    for f in files:\n",
    "        if not f.exists():\n",
    "            print(f\"⚠️  {f} not found, skipping\")\n",
    "            continue\n",
    "        ds = xr.open_dataset(f)\n",
    "        missing = set(INPUT_FEATURES + TARGET_FEATURES) - set(ds.data_vars)\n",
    "        if missing:\n",
    "            print(f\"⚠️  {f.name}: missing {missing}, skipping\")\n",
    "            continue\n",
    "        ds.attrs[\"year\"] = int(f.name.split(\"_\")[-1].split(\".\")[0])\n",
    "        datasets.append(ds)\n",
    "        print(f\"Loaded {f.name}: {len(ds.valid_time)} timesteps\")\n",
    "    if not datasets:\n",
    "        raise FileNotFoundError(\"No valid datasets found for requested years\")\n",
    "\n",
    "    combined = xr.concat(datasets, dim=\"valid_time\").sortby(\"valid_time\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def split_by_year(ds, train_years, val_year):\n",
    "    train_mask = ds.valid_time.dt.year.isin(train_years)\n",
    "    val_mask = ds.valid_time.dt.year == val_year\n",
    "    return ds.isel(valid_time=train_mask), ds.isel(valid_time=val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa786da-4402-4379-89b4-8038782c3856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREPROCESSING helpers\n",
    "\"\"\"\n",
    "\n",
    "def create_ocean_mask(ds, threshold=0.5, ref_var=\"shts\"):\n",
    "    arr = ds[ref_var].values\n",
    "    nan_ratio = np.isnan(arr).sum(axis=0) / arr.shape[0]\n",
    "    mask = nan_ratio < threshold\n",
    "    print(f\"Ocean points: {mask.sum()}/{mask.size} ({100*mask.sum()/mask.size:.1f}%)\")\n",
    "    return mask\n",
    "\n",
    "\n",
    "def engineer_frames(ds, ocean_mask, input_feats=INPUT_FEATURES, target_feats=TARGET_FEATURES):\n",
    "    # shape: (time, lat, lon, channel)\n",
    "    X = ds[input_feats].to_array(dim=\"channel\").transpose(\"valid_time\", \"latitude\", \"longitude\", \"channel\").values\n",
    "    y = ds[target_feats].to_array(dim=\"channel\").transpose(\"valid_time\", \"latitude\", \"longitude\", \"channel\").values\n",
    "\n",
    "    # broadcast mask and zero-out land\n",
    "    mask_3d = np.broadcast_to(ocean_mask[..., np.newaxis], X.shape[1:])\n",
    "    X[:, ~mask_3d] = 0.0\n",
    "    y[:, ~mask_3d] = 0.0\n",
    "\n",
    "    # fill NaNs with channel means\n",
    "    for i in range(X.shape[-1]):\n",
    "        ch = X[..., i]\n",
    "        mean = np.nanmean(ch)\n",
    "        X[..., i] = np.nan_to_num(ch, nan=mean)\n",
    "    for i in range(y.shape[-1]):\n",
    "        ch = y[..., i]\n",
    "        mean = np.nanmean(ch)\n",
    "        y[..., i] = np.nan_to_num(ch, nan=mean)\n",
    "\n",
    "    return X.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "\n",
    "def fit_scalers(X_train, y_train):\n",
    "    scalers_X, scalers_y = [], []\n",
    "    # Flatten each channel across time+space\n",
    "    for i in range(X_train.shape[-1]):\n",
    "        s = StandardScaler()\n",
    "        s.fit(X_train[..., i].reshape(-1, 1))\n",
    "        scalers_X.append(s)\n",
    "    for i in range(y_train.shape[-1]):\n",
    "        s = StandardScaler()\n",
    "        s.fit(y_train[..., i].reshape(-1, 1))\n",
    "        scalers_y.append(s)\n",
    "    return scalers_X, scalers_y\n",
    "\n",
    "\n",
    "def apply_scalers(X, y, scalers_X, scalers_y):\n",
    "    Xs = np.zeros_like(X)\n",
    "    ys = np.zeros_like(y)\n",
    "    for i, s in enumerate(scalers_X):\n",
    "        Xs[..., i] = s.transform(X[..., i].reshape(-1, 1)).reshape(X[..., i].shape)\n",
    "    for i, s in enumerate(scalers_y):\n",
    "        ys[..., i] = s.transform(y[..., i].reshape(-1, 1)).reshape(y[..., i].shape)\n",
    "    return Xs, ys\n",
    "\n",
    "\n",
    "def create_sequences(X, y, lookback, lookahead):\n",
    "    n = len(X) - lookback - lookahead + 1\n",
    "    X_seq = np.zeros((n, lookback, *X.shape[1:]), dtype=np.float32)\n",
    "    y_seq = np.zeros((n, *y.shape[1:]), dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        X_seq[i] = X[i : i + lookback]\n",
    "        y_seq[i] = y[i + lookback + lookahead - 1]\n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285b669-38df-4b2d-9dc9-d6c02f53ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SAVE / LOAD preprocessed\n",
    "\"\"\"\n",
    "def save_preprocessed(path, **kwargs):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(kwargs, f, protocol=4)\n",
    "    print(f\"Saved preprocessed -> {path}\")\n",
    "\n",
    "\n",
    "def load_preprocessed(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece2cae-8818-4591-b81a-0353fb3fb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "MODEL definition\n",
    "\"\"\"\n",
    "def build_enhanced_model(input_shape, output_channels, lr=LEARNING_RATE):\n",
    "    i = Input(shape=input_shape)\n",
    "    x = ConvLSTM2D(128, (3, 3), padding=\"same\", return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(1e-5))(i)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ConvLSTM2D(32, (3, 3), padding=\"same\", return_sequences=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv3D(output_channels, kernel_size=(3, 3, 3), padding=\"same\", activation=\"linear\")(x)\n",
    "    out = x[:, -1, :, :, :]\n",
    "    m = Model(i, out)\n",
    "    m.compile(optimizer=Adam(learning_rate=lr), loss=\"mse\", metrics=[\"mae\"]) \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a35ba4-a5bc-45f2-accf-7b1714899a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EVALUATION helpers\n",
    "\"\"\"\n",
    "def evaluate_model(model, X_val, y_val, scalers_y, target_names=TARGET_FEATURES):\n",
    "    yp_scaled = model.predict(X_val, batch_size=32, verbose=1)\n",
    "    y_true = np.zeros_like(y_val)\n",
    "    y_pred = np.zeros_like(yp_scaled)\n",
    "    for i, s in enumerate(scalers_y):\n",
    "        y_true[..., i] = s.inverse_transform(y_val[..., i].reshape(-1, 1)).reshape(y_val[..., i].shape)\n",
    "        y_pred[..., i] = s.inverse_transform(yp_scaled[..., i].reshape(-1, 1)).reshape(yp_scaled[..., i].shape)\n",
    "\n",
    "    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "    rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))\n",
    "    print(f\"Overall MAE={mae:.4f}, RMSE={rmse:.4f}\")\n",
    "\n",
    "    for i, name in enumerate(target_names):\n",
    "        mae_v = mean_absolute_error(y_true[..., i].flatten(), y_pred[..., i].flatten())\n",
    "        rmse_v = np.sqrt(mean_squared_error(y_true[..., i].flatten(), y_pred[..., i].flatten()))\n",
    "        r2_v = r2_score(y_true[..., i].flatten(), y_pred[..., i].flatten())\n",
    "        print(f\"{name:>6s}: MAE={mae_v:.3f}, RMSE={rmse_v:.3f}, R2={r2_v:.3f}\")\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f738e0d-f3dc-482a-b4eb-1d3a079157d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "NEAREST OCEAN POINT\n",
    "\"\"\"\n",
    "def find_nearest_ocean_point(ds, lat, lon, ref_var=\"shts\"):\n",
    "    pt = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "    if not np.isnan(pt[ref_var].isel(valid_time=0)):\n",
    "        return float(pt.latitude), float(pt.longitude)\n",
    "    shts = ds[ref_var].isel(valid_time=0).stack(point=(\"latitude\", \"longitude\")).dropna(\"point\")\n",
    "    lat1, lon1 = np.radians(lat), np.radians(lon)\n",
    "    lat2 = np.radians(shts.latitude)\n",
    "    lon2 = np.radians(shts.longitude)\n",
    "    dlat = (lat2 - lat1) / 2\n",
    "    dlon = (lon2 - lon1) / 2\n",
    "    a = np.sin(dlat) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon) ** 2\n",
    "    R = 6371\n",
    "    distance = 2 * R * np.arcsin(np.sqrt(a))\n",
    "    idx = distance.argmin()\n",
    "    closest = shts.isel(point=idx)\n",
    "    return float(closest.latitude), float(closest.longitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731671c-7424-4554-8b6f-6adb5f515cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MAIN pipeline (call cells step-by-step in notebook)\n",
    "\n",
    "Recommended notebook workflow:\n",
    " 1) Run data loading cell\n",
    " 2) Run preprocessing cell\n",
    " 3) Run model build + train cell\n",
    " 4) Run evaluation + forecast cell\n",
    "\n",
    "This keeps each step inspectable in the notebook.\n",
    "\"\"\"\n",
    "\n",
    "# Cell: LOAD data\n",
    "if __name__ == \"__main__\":\n",
    "    # This block is safe to run as a linear script. In a notebook, run the cells individually.\n",
    "    # -- load or preprocess\n",
    "    if LOAD_PREPROCESSED and Path(PREPROCESSED_FILE).exists():\n",
    "        data = load_preprocessed(PREPROCESSED_FILE)\n",
    "        X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "        X_val, y_val = data[\"X_val\"], data[\"y_val\"]\n",
    "        scalers_X, scalers_y = data[\"scalers_X\"], data[\"scalers_y\"]\n",
    "        ds = load_multiyear_dataset(DATA_YEARS, DATA_DIR)\n",
    "        print(\"Loaded preprocessed data\")\n",
    "    else:\n",
    "        ds = load_multiyear_dataset(DATA_YEARS, DATA_DIR)\n",
    "        ds_train, ds_val = split_by_year(ds, TRAIN_YEARS, VAL_YEAR)\n",
    "        ocean_mask = create_ocean_mask(ds)\n",
    "        X_train, y_train = engineer_frames(ds_train, ocean_mask)\n",
    "        X_val, y_val = engineer_frames(ds_val, ocean_mask)\n",
    "        scalers_X, scalers_y = fit_scalers(X_train, y_train)\n",
    "        X_train, y_train = apply_scalers(X_train, y_train, scalers_X, scalers_y)\n",
    "        X_val, y_val = apply_scalers(X_val, y_val, scalers_X, scalers_y)\n",
    "        X_train, y_train = create_sequences(X_train, y_train, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "        X_val, y_val = create_sequences(X_val, y_val, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "        save_preprocessed(PREPROCESSED_FILE, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, scalers_X=scalers_X, scalers_y=scalers_y)\n",
    "\n",
    "    print(f\"Train shapes: {X_train.shape}, {y_train.shape} | Val shapes: {X_val.shape}, {y_val.shape}\")\n",
    "\n",
    "    # Build\n",
    "    model = build_enhanced_model(input_shape=X_train.shape[1:], output_channels=y_train.shape[-1])\n",
    "    model.summary()\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=2, min_lr=1e-7, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=1)\n",
    "    model.save(MODEL_SAVE_PATH)\n",
    "\n",
    "    # Evaluate\n",
    "    y_true, y_pred = evaluate_model(model, X_val, y_val, scalers_y)\n",
    "\n",
    "    # Forecast example\n",
    "    buoy_lat, buoy_lon = find_nearest_ocean_point(ds, SPOT_LAT, SPOT_LON)\n",
    "    last_seq = X_val[-1:]\n",
    "    fc_scaled = model.predict(last_seq, verbose=0)[0]\n",
    "    fc = np.zeros_like(fc_scaled)\n",
    "    for i, s in enumerate(scalers_y):\n",
    "        fc[..., i] = s.inverse_transform(fc_scaled[..., i].reshape(-1, 1)).reshape(fc_scaled[..., i].shape)\n",
    "    lats = ds.latitude.values\n",
    "    lons = ds.longitude.values\n",
    "    lat_idx = np.argmin(np.abs(lats - buoy_lat))\n",
    "    lon_idx = np.argmin(np.abs(lons - buoy_lon))\n",
    "    bf = fc[lat_idx, lon_idx, :]\n",
    "    print(\"Forecast (example):\")\n",
    "    print(f\"  Swell Height: {bf[0]:.2f} m | Period: {bf[1]:.2f} s | Direction: {bf[2]:.1f}°\")\n",
    "\n",
    "# End of notebook-like script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
