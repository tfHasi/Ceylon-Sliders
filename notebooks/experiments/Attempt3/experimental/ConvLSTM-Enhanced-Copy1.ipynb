{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4382119e-a348-4ba5-9f80-c96a23e678cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Year ConvLSTM Wave Forecasting Pipeline\n",
    "# Using 2020-2024 ERA5 Data\n",
    "# ======================================================================\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    ConvLSTM2D, Conv3D, BatchNormalization, Input, Dropout\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIGURATION\n",
    "DATA_YEARS = [2020, 2021, 2022, 2023, 2024, 2025]\n",
    "DATA_DIR = \"../\"  # Directory containing surf_data_YYYY.nc files\n",
    "SPOT_LAT, SPOT_LON = 6.8399, 81.8396\n",
    "\n",
    "# Training config\n",
    "LOOKBACK_HOURS = 19\n",
    "LOOKAHEAD_HOURS = 1\n",
    "BATCH_SIZE = 16  # Can increase with more data\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Data split strategy\n",
    "TRAIN_YEARS = [2020, 2021, 2022, 2023]  # 4 years training\n",
    "VAL_YEAR = 2024  # Hold out 2024 for validation (most recent)\n",
    "\n",
    "INPUT_FEATURES = [\"u10\", \"v10\", \"msl\", \"shts\", \"mpts\", \"mdts\"]\n",
    "TARGET_FEATURES = [\"shts\", \"mpts\", \"mdts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c85bc9-1342-4d55-9df7-41d21b78416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-YEAR DATA LOADING\n",
    "# ======================================================================\n",
    "def load_multiyear_dataset(years, data_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Load and concatenate multiple years of ERA5 data.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING MULTI-YEAR DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    datasets = []\n",
    "    for year in years:\n",
    "        filepath = Path(data_dir) / f\"surf_data_{year}.nc\"\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            print(f\"‚ö†Ô∏è  Warning: {filepath} not found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            ds = xr.open_dataset(filepath)\n",
    "            \n",
    "            # Verify required variables exist\n",
    "            missing_vars = set(INPUT_FEATURES + TARGET_FEATURES) - set(ds.data_vars)\n",
    "            if missing_vars:\n",
    "                print(f\"‚ö†Ô∏è  {year}: Missing variables {missing_vars}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Add year attribute for tracking\n",
    "            ds.attrs['year'] = year\n",
    "            datasets.append(ds)\n",
    "            \n",
    "            print(f\"‚úì Loaded {year}: {len(ds.valid_time)} timesteps, \"\n",
    "                  f\"shape={ds[INPUT_FEATURES[0]].shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {year}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not datasets:\n",
    "        raise ValueError(\"No valid datasets loaded!\")\n",
    "    \n",
    "    # Concatenate along time dimension\n",
    "    print(\"\\nüîó Concatenating datasets...\")\n",
    "    combined_ds = xr.concat(datasets, dim='valid_time')\n",
    "    \n",
    "    # Sort by time (important!)\n",
    "    combined_ds = combined_ds.sortby('valid_time')\n",
    "    \n",
    "    print(f\"\\n‚úì Combined dataset:\")\n",
    "    print(f\"   Total timesteps: {len(combined_ds.valid_time)}\")\n",
    "    print(f\"   Time range: {combined_ds.valid_time.values[0]} to {combined_ds.valid_time.values[-1]}\")\n",
    "    print(f\"   Spatial shape: {len(combined_ds.latitude)} √ó {len(combined_ds.longitude)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return combined_ds\n",
    "\n",
    "\n",
    "def split_by_year(ds, train_years, val_years):\n",
    "    \"\"\"\n",
    "    Split dataset by year (better than random split for time series).\n",
    "    \"\"\"\n",
    "    train_mask = ds.valid_time.dt.year.isin(train_years)\n",
    "    val_mask = ds.valid_time.dt.year.isin(val_years)\n",
    "    \n",
    "    ds_train = ds.isel(valid_time=train_mask)\n",
    "    ds_val = ds.isel(valid_time=val_mask)\n",
    "    \n",
    "    print(f\"\\nüìä Data Split:\")\n",
    "    print(f\"   Training years: {train_years} ‚Üí {len(ds_train.valid_time)} samples\")\n",
    "    print(f\"   Validation years: {val_years} ‚Üí {len(ds_val.valid_time)} samples\")\n",
    "    print(f\"   Split ratio: {len(ds_train.valid_time)/(len(ds_train.valid_time)+len(ds_val.valid_time)):.1%} train\")\n",
    "    \n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9c2ff8-474f-4422-b5a1-328dcc431976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED PREPROCESSING FOR MULTI-YEAR DATA\n",
    "# ======================================================================\n",
    "def create_ocean_mask(ds, threshold=0.5):\n",
    "    \"\"\"Create persistent ocean mask across all years.\"\"\"\n",
    "    shts_data = ds[\"shts\"].values\n",
    "    nan_ratio = np.isnan(shts_data).sum(axis=0) / shts_data.shape[0]\n",
    "    ocean_mask = nan_ratio < threshold\n",
    "    \n",
    "    valid_points = ocean_mask.sum()\n",
    "    print(f\"\\nüåä Ocean Mask: {valid_points}/{ocean_mask.size} points ({100*valid_points/ocean_mask.size:.1f}%)\")\n",
    "    return ocean_mask\n",
    "\n",
    "\n",
    "def engineer_frames(ds, ocean_mask):\n",
    "    \"\"\"Extract and prepare spatial frames.\"\"\"\n",
    "    X = ds[INPUT_FEATURES].to_array(dim=\"channel\").transpose(\n",
    "        \"valid_time\", \"latitude\", \"longitude\", \"channel\"\n",
    "    ).values\n",
    "    \n",
    "    y = ds[TARGET_FEATURES].to_array(dim=\"channel\").transpose(\n",
    "        \"valid_time\", \"latitude\", \"longitude\", \"channel\"\n",
    "    ).values\n",
    "    \n",
    "    # Apply ocean mask\n",
    "    mask_3d_X = np.broadcast_to(ocean_mask[..., np.newaxis], X.shape[1:])\n",
    "    mask_3d_y = np.broadcast_to(ocean_mask[..., np.newaxis], y.shape[1:])\n",
    "    \n",
    "    X[:, ~mask_3d_X] = 0.0\n",
    "    y[:, ~mask_3d_y] = 0.0\n",
    "    \n",
    "    # Fill NaNs with channel-wise means\n",
    "    for i in range(X.shape[-1]):\n",
    "        channel_mean = np.nanmean(X[..., i])\n",
    "        X[..., i] = np.nan_to_num(X[..., i], nan=channel_mean)\n",
    "    \n",
    "    for i in range(y.shape[-1]):\n",
    "        channel_mean = np.nanmean(y[..., i])\n",
    "        y[..., i] = np.nan_to_num(y[..., i], nan=channel_mean)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fit_scalers(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fit scalers ONLY on training data to prevent data leakage.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìè Fitting scalers on training data...\")\n",
    "    \n",
    "    scalers_X = []\n",
    "    for i in range(X_train.shape[-1]):\n",
    "        scaler = StandardScaler()\n",
    "        channel = X_train[..., i].reshape(-1, 1)\n",
    "        scaler.fit(channel)\n",
    "        scalers_X.append(scaler)\n",
    "    \n",
    "    scalers_y = []\n",
    "    for i in range(y_train.shape[-1]):\n",
    "        scaler = StandardScaler()\n",
    "        channel = y_train[..., i].reshape(-1, 1)\n",
    "        scaler.fit(channel)\n",
    "        scalers_y.append(scaler)\n",
    "    \n",
    "    return scalers_X, scalers_y\n",
    "\n",
    "\n",
    "def apply_scalers(X, y, scalers_X, scalers_y):\n",
    "    \"\"\"Apply pre-fitted scalers.\"\"\"\n",
    "    X_scaled = np.zeros_like(X)\n",
    "    y_scaled = np.zeros_like(y)\n",
    "    \n",
    "    for i in range(X.shape[-1]):\n",
    "        channel = X[..., i].reshape(-1, 1)\n",
    "        X_scaled[..., i] = scalers_X[i].transform(channel).reshape(X[..., i].shape)\n",
    "    \n",
    "    for i in range(y.shape[-1]):\n",
    "        channel = y[..., i].reshape(-1, 1)\n",
    "        y_scaled[..., i] = scalers_y[i].transform(channel).reshape(y[..., i].shape)\n",
    "    \n",
    "    return X_scaled, y_scaled\n",
    "\n",
    "\n",
    "def create_sequences(X, y, lookback, lookahead):\n",
    "    \"\"\"Generate sequences with progress tracking.\"\"\"\n",
    "    total_sequences = len(X) - lookback - lookahead + 1\n",
    "    print(f\"\\nüîÑ Creating {total_sequences} sequences...\")\n",
    "    \n",
    "    X_seq = np.zeros((total_sequences, lookback, *X.shape[1:]), dtype=np.float32)\n",
    "    y_seq = np.zeros((total_sequences, *y.shape[1:]), dtype=np.float32)\n",
    "    \n",
    "    for i in range(total_sequences):\n",
    "        X_seq[i] = X[i:i + lookback]\n",
    "        y_seq[i] = y[i + lookback + lookahead - 1]\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"   Progress: {i+1}/{total_sequences} ({100*(i+1)/total_sequences:.1f}%)\")\n",
    "    \n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7293fd28-22c4-4630-aade-da6a50d55a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE/LOAD PREPROCESSED DATA\n",
    "# ======================================================================\n",
    "def save_preprocessed_data(X_train, y_train, X_val, y_val, scalers_X, scalers_y, filename=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Save preprocessed data to avoid reprocessing.\"\"\"\n",
    "    print(f\"\\nüíæ Saving preprocessed data to {filename}...\")\n",
    "    \n",
    "    data = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'scalers_X': scalers_X,\n",
    "        'scalers_y': scalers_y\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=4)\n",
    "    \n",
    "    print(f\"‚úì Saved {filename} ({Path(filename).stat().st_size / 1e6:.1f} MB)\")\n",
    "\n",
    "\n",
    "def load_preprocessed_data(filename=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Load preprocessed data.\"\"\"\n",
    "    print(f\"\\nüìÇ Loading preprocessed data from {filename}...\")\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(\"‚úì Loaded preprocessed data\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d3fa47-cfdc-45f9-9f50-44dd90612128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED MODEL ARCHITECTURE\n",
    "# ======================================================================\n",
    "def build_enhanced_model(input_shape, output_channels):\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder with increasing capacity\n",
    "    x = ConvLSTM2D(\n",
    "        128, (3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    )(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = ConvLSTM2D(\n",
    "        64, (3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = ConvLSTM2D(\n",
    "        32, (3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=True\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Conv3D prediction layer\n",
    "    x = Conv3D(\n",
    "        output_channels,\n",
    "        kernel_size=(3, 3, 3),\n",
    "        padding='same',\n",
    "        activation='linear'\n",
    "    )(x)\n",
    "    \n",
    "    # Extract last timestep\n",
    "    outputs = x[:, -1, :, :, :]\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67fea05-d538-4aab-8404-7ed0b73f6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "# ======================================================================\n",
    "def evaluate_model(model, X_val, y_val, scalers_y, target_names):\n",
    "    \"\"\"Comprehensive evaluation.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    y_pred_scaled = model.predict(X_val, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_true = np.zeros_like(y_val)\n",
    "    y_pred = np.zeros_like(y_pred_scaled)\n",
    "    \n",
    "    for i in range(y_val.shape[-1]):\n",
    "        y_true[..., i] = scalers_y[i].inverse_transform(\n",
    "            y_val[..., i].reshape(-1, 1)\n",
    "        ).reshape(y_val[..., i].shape)\n",
    "        y_pred[..., i] = scalers_y[i].inverse_transform(\n",
    "            y_pred_scaled[..., i].reshape(-1, 1)\n",
    "        ).reshape(y_pred_scaled[..., i].shape)\n",
    "    \n",
    "    # Overall metrics\n",
    "    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "    rmse = np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))\n",
    "    \n",
    "    print(f\"\\nOverall Spatial Metrics:\")\n",
    "    print(f\"   MAE:  {mae:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Per-variable metrics\n",
    "    print(f\"\\nPer-Variable Metrics:\")\n",
    "    for i, var in enumerate(target_names):\n",
    "        mae_var = mean_absolute_error(y_true[..., i].flatten(), y_pred[..., i].flatten())\n",
    "        rmse_var = np.sqrt(mean_squared_error(y_true[..., i].flatten(), y_pred[..., i].flatten()))\n",
    "        r2_var = r2_score(y_true[..., i].flatten(), y_pred[..., i].flatten())\n",
    "        \n",
    "        print(f\"   {var:>6s}: MAE={mae_var:.3f}, RMSE={rmse_var:.3f}, R¬≤={r2_var:.3f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def find_nearest_ocean_point(ds, lat, lon):\n",
    "    \"\"\"Find nearest valid ocean point.\"\"\"\n",
    "    nearest = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "    \n",
    "    if not np.isnan(nearest[\"shts\"].isel(valid_time=0)):\n",
    "        return float(nearest.latitude), float(nearest.longitude)\n",
    "    \n",
    "    shts_data = ds[\"shts\"].isel(valid_time=0).stack(\n",
    "        point=(\"latitude\", \"longitude\")\n",
    "    ).dropna(\"point\")\n",
    "    \n",
    "    R = 6371\n",
    "    lat1, lon1 = np.radians(lat), np.radians(lon)\n",
    "    lat2 = np.radians(shts_data.latitude)\n",
    "    lon2 = np.radians(shts_data.longitude)\n",
    "    \n",
    "    dlat = (lat2 - lat1) / 2\n",
    "    dlon = (lon2 - lon1) / 2\n",
    "    \n",
    "    a = np.sin(dlat)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon)**2\n",
    "    distance = 2 * R * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    closest = shts_data.isel(point=distance.argmin())\n",
    "    return float(closest.latitude), float(closest.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e5b088-daaa-4b6b-a900-75c8347abc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä MULTI-YEAR CONVLSTM WAVE FORECASTING\n",
      "======================================================================\n",
      "======================================================================\n",
      "LOADING MULTI-YEAR DATASET\n",
      "======================================================================\n",
      "‚úì Loaded 2020: 1464 timesteps, shape=(1464, 21, 21)\n",
      "‚úì Loaded 2021: 1460 timesteps, shape=(1460, 21, 21)\n",
      "‚úì Loaded 2022: 1460 timesteps, shape=(1460, 21, 21)\n",
      "‚úì Loaded 2023: 1460 timesteps, shape=(1460, 21, 21)\n",
      "‚úì Loaded 2024: 1464 timesteps, shape=(1464, 21, 21)\n",
      "‚úì Loaded 2025: 1283 timesteps, shape=(1283, 21, 21)\n",
      "\n",
      "üîó Concatenating datasets...\n",
      "\n",
      "‚úì Combined dataset:\n",
      "   Total timesteps: 8591\n",
      "   Time range: 2020-01-01T00:00:00.000000000 to 2025-11-17T12:00:00.000000000\n",
      "   Spatial shape: 21 √ó 21\n",
      "======================================================================\n",
      "\n",
      "üìä Data Split:\n",
      "   Training years: [2020, 2021, 2022, 2023] ‚Üí 5844 samples\n",
      "   Validation years: [2024] ‚Üí 1464 samples\n",
      "   Split ratio: 80.0% train\n",
      "\n",
      "üåä Ocean Mask: 93/441 points (21.1%)\n",
      "\n",
      "üìä Engineering features...\n",
      "   Train: X=(5844, 21, 21, 6), y=(5844, 21, 21, 3)\n",
      "   Val:   X=(1464, 21, 21, 6), y=(1464, 21, 21, 3)\n",
      "\n",
      "üìè Fitting scalers on training data...\n",
      "\n",
      "üîß Applying scalers...\n",
      "\n",
      "üîÑ Creating 5825 sequences...\n",
      "   Progress: 500/5825 (8.6%)\n",
      "   Progress: 1000/5825 (17.2%)\n",
      "   Progress: 1500/5825 (25.8%)\n",
      "   Progress: 2000/5825 (34.3%)\n",
      "   Progress: 2500/5825 (42.9%)\n",
      "   Progress: 3000/5825 (51.5%)\n",
      "   Progress: 3500/5825 (60.1%)\n",
      "   Progress: 4000/5825 (68.7%)\n",
      "   Progress: 4500/5825 (77.3%)\n",
      "   Progress: 5000/5825 (85.8%)\n",
      "   Progress: 5500/5825 (94.4%)\n",
      "\n",
      "üîÑ Creating 1445 sequences...\n",
      "   Progress: 500/1445 (34.6%)\n",
      "   Progress: 1000/1445 (69.2%)\n",
      "\n",
      "‚úì Final shapes:\n",
      "   Train: X=(5825, 19, 21, 21, 6), y=(5825, 21, 21, 3)\n",
      "   Val:   X=(1445, 19, 21, 21, 6), y=(1445, 21, 21, 3)\n",
      "\n",
      "üíæ Saving preprocessed data to preprocessed_multiyear.pkl...\n",
      "‚úì Saved preprocessed_multiyear.pkl (1500.4 MB)\n"
     ]
    }
   ],
   "source": [
    "# MAIN PIPELINE\n",
    "def main():\n",
    "    print(\"üåä MULTI-YEAR CONVLSTM WAVE FORECASTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if preprocessed data exists\n",
    "    preprocessed_file = \"preprocessed_multiyear.pkl\"\n",
    "    \n",
    "    if Path(preprocessed_file).exists():\n",
    "        print(f\"\\n‚úì Found {preprocessed_file}\")\n",
    "        user_input = input(\"Load preprocessed data? (y/n): \")\n",
    "        \n",
    "        if user_input.lower() == 'y':\n",
    "            data = load_preprocessed_data(preprocessed_file)\n",
    "            X_train, y_train = data['X_train'], data['y_train']\n",
    "            X_val, y_val = data['X_val'], data['y_val']\n",
    "            scalers_X, scalers_y = data['scalers_X'], data['scalers_y']\n",
    "            \n",
    "            print(f\"‚úì Loaded: Train={X_train.shape}, Val={X_val.shape}\")\n",
    "            \n",
    "            # Still need to load ds for metadata\n",
    "            ds = load_multiyear_dataset(DATA_YEARS, DATA_DIR)\n",
    "            \n",
    "            skip_preprocessing = True\n",
    "        else:\n",
    "            skip_preprocessing = False\n",
    "    else:\n",
    "        skip_preprocessing = False\n",
    "    \n",
    "    if not skip_preprocessing:\n",
    "        # Step 1: Load all years\n",
    "        ds = load_multiyear_dataset(DATA_YEARS, DATA_DIR)\n",
    "        \n",
    "        # Step 2: Split by year\n",
    "        ds_train, ds_val = split_by_year(ds, TRAIN_YEARS, [VAL_YEAR])\n",
    "        \n",
    "        # Step 3: Create ocean mask (from combined data)\n",
    "        ocean_mask = create_ocean_mask(ds)\n",
    "        \n",
    "        # Step 4: Engineer features\n",
    "        print(\"\\nüìä Engineering features...\")\n",
    "        X_train, y_train = engineer_frames(ds_train, ocean_mask)\n",
    "        X_val, y_val = engineer_frames(ds_val, ocean_mask)\n",
    "        \n",
    "        print(f\"   Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "        print(f\"   Val:   X={X_val.shape}, y={y_val.shape}\")\n",
    "        \n",
    "        # Step 5: Fit scalers on training data only\n",
    "        scalers_X, scalers_y = fit_scalers(X_train, y_train)\n",
    "        \n",
    "        # Step 6: Apply scalers\n",
    "        print(\"\\nüîß Applying scalers...\")\n",
    "        X_train, y_train = apply_scalers(X_train, y_train, scalers_X, scalers_y)\n",
    "        X_val, y_val = apply_scalers(X_val, y_val, scalers_X, scalers_y)\n",
    "        \n",
    "        # Step 7: Create sequences\n",
    "        X_train, y_train = create_sequences(X_train, y_train, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "        X_val, y_val = create_sequences(X_val, y_val, LOOKBACK_HOURS, LOOKAHEAD_HOURS)\n",
    "        \n",
    "        print(f\"\\n‚úì Final shapes:\")\n",
    "        print(f\"   Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "        print(f\"   Val:   X={X_val.shape}, y={y_val.shape}\")\n",
    "        \n",
    "        # Save for future runs\n",
    "        save_preprocessed_data(X_train, y_train, X_val, y_val, scalers_X, scalers_y, preprocessed_file)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
