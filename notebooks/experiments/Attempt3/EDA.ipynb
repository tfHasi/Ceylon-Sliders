{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32099550-f8a9-441f-9561-f8fdfe5082b9",
   "metadata": {},
   "source": [
    "### ERA5 Dataset \n",
    "The server splits the data you requested into separate files based on the type of variable and the model stream it comes from.\n",
    "Your request included variables from different categories, which the system packages separately. Hereâ€™s the breakdown:\n",
    "\n",
    "data_stream-oper_stepType-instant.nc (Atmospheric Instantaneous) : This file contains variables that represent a snapshot in time.\n",
    "Variables: 10m_u_component_of_wind, 10m_v_component_of_wind, mean_sea_level_pressure, sea_surface_temperature, total_cloud_cover.\n",
    "\n",
    "data_stream-oper_stepType-accum.nc (Atmospheric Accumulated) : This file contains variables that are accumulated or averaged over a time period. total_precipitation isn't an instantaneous value; it's the total rain that fell in the hours leading up to the timestamp (e.g., total rainfall between 00:00 and 06:00).\n",
    "Variables: total_precipitation.\n",
    "\n",
    "data_stream-wave_stepType-instant.nc (Wave Model Instantaneous) : This file contains variables generated by a separate wave model (WAM), not the primary atmospheric model.\n",
    "Variables: significant_height_of_combined_wind_waves_and_swell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44544081-81fa-4a06-a515-1f8cebfd3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import zipfile\n",
    "import os\n",
    "import glob\n",
    "\n",
    "YEAR = \"2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36306364-e03e-48d6-86dd-d168eff23a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted: surf_data_2024_01.nc\n",
      "  Extracted: surf_data_2024_02.nc\n",
      "  Extracted: surf_data_2024_03.nc\n",
      "  Extracted: surf_data_2024_04.nc\n",
      "  Extracted: surf_data_2024_05.nc\n",
      "  Extracted: surf_data_2024_06.nc\n",
      "  Extracted: surf_data_2024_07.nc\n",
      "  Extracted: surf_data_2024_08.nc\n",
      "  Extracted: surf_data_2024_09.nc\n",
      "  Extracted: surf_data_2024_10.nc\n",
      "  Extracted: surf_data_2024_11.nc\n",
      "  Extracted: surf_data_2024_12.nc\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT ALL MONTHLY ARCHIVES\n",
    "for month_num in range(1, 13):\n",
    "    month_str = f\"{month_num:02d}\"\n",
    "    # Define the path to the monthly archive and the directory to extract into\n",
    "    extract_path = os.path.join(YEAR, month_str)\n",
    "    archive_path = os.path.join(extract_path, f\"surf_data_{YEAR}_{month_str}.nc\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(archive_path, 'r') as zip_file:\n",
    "            zip_file.extractall(path=extract_path)\n",
    "        print(f\"  Extracted: {os.path.basename(archive_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {os.path.basename(archive_path)}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450b21bd-5918-467c-b2e4-0601c35e8eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing directory: 2024\\01\n",
      "  Processing directory: 2024\\02\n",
      "  Processing directory: 2024\\03\n",
      "  Processing directory: 2024\\04\n",
      "  Processing directory: 2024\\05\n",
      "  Processing directory: 2024\\06\n",
      "  Processing directory: 2024\\07\n",
      "  Processing directory: 2024\\08\n",
      "  Processing directory: 2024\\09\n",
      "  Processing directory: 2024\\10\n",
      "  Processing directory: 2024\\11\n",
      "  Processing directory: 2024\\12\n"
     ]
    }
   ],
   "source": [
    "# MERGE EXTRACTED FILES\n",
    "list_of_monthly_datasets = []\n",
    "\n",
    "for month_num in range(1, 13):\n",
    "    month_str = f\"{month_num:02d}\"\n",
    "    month_directory = os.path.join(YEAR, month_str)\n",
    "    \n",
    "    print(f\"  Processing directory: {month_directory}\")\n",
    "\n",
    "    # Define the expected data_stream files within the directory\n",
    "    files_to_merge = [\n",
    "        os.path.join(month_directory, \"data_stream-oper_stepType-instant.nc\"),\n",
    "        os.path.join(month_directory, \"data_stream-oper_stepType-accum.nc\"),\n",
    "        os.path.join(month_directory, \"data_stream-wave_stepType-instant.nc\"),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        datasets = []\n",
    "        # Load each data_stream file that exists\n",
    "        for file_path in files_to_merge:\n",
    "            if os.path.exists(file_path):\n",
    "                ds_part = xr.open_dataset(file_path)\n",
    "                datasets.append(ds_part)\n",
    "        \n",
    "        if not datasets:\n",
    "            print(f\"    -> No data_stream files found to merge. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        combined_ds = xr.merge(datasets, compat='override')\n",
    "        cleaned_ds = combined_ds.dropna(dim=\"valid_time\", how=\"all\")\n",
    "        list_of_monthly_datasets.append(cleaned_ds)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f65f78fe-a563-4542-8ea1-fafb02e426fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 16MB\n",
      "Dimensions:     (valid_time: 1464, latitude: 21, longitude: 21)\n",
      "Coordinates:\n",
      "  * valid_time  (valid_time) datetime64[ns] 12kB 2024-01-01 ... 2024-12-31T18...\n",
      "  * latitude    (latitude) float64 168B 5.0 5.25 5.5 5.75 ... 9.25 9.5 9.75 10.0\n",
      "  * longitude   (longitude) float64 168B 78.0 78.25 78.5 ... 82.5 82.75 83.0\n",
      "    number      int64 8B 0\n",
      "    expver      (valid_time) <U4 23kB '0001' '0001' '0001' ... '0001' '0001'\n",
      "Data variables:\n",
      "    u10         (valid_time, latitude, longitude) float32 3MB -4.375 ... -6.738\n",
      "    v10         (valid_time, latitude, longitude) float32 3MB -1.539 ... -6.051\n",
      "    msl         (valid_time, latitude, longitude) float32 3MB 1.011e+05 ... 1...\n",
      "    shts        (valid_time, latitude, longitude) float32 3MB 1.488 ... 1.261\n",
      "    mpts        (valid_time, latitude, longitude) float32 3MB 7.378 ... 7.853\n",
      "    mdts        (valid_time, latitude, longitude) float32 3MB 79.67 ... 120.2\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-10-26T13:55 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Concatenate all the monthly datasets into one for the entire year\n",
    "    ds = xr.concat(list_of_monthly_datasets, dim=\"valid_time\")\n",
    "    print(ds)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a70e27f-2b54-499f-82ff-82f87e4c323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to final csv\n",
    "output_filename = f\"surf_data_{YEAR}.nc\"\n",
    "ds.to_netcdf(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc63664-0e5c-4c2a-91fd-5fc9c654407b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m ds_month \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(output_filename)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check for excessive NaNs\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvariables\u001b[49m:\n\u001b[0;32m      6\u001b[0m     nan_pct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(ds_month[var]\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m ds_month[var]\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nan_pct \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m20\u001b[39m:  \u001b[38;5;66;03m# Flag if >20% NaNs\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'variables' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
